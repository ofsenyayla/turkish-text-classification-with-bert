{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 952
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "gT_Wl3fTsmTR",
    "outputId": "b8e17e55-d55e-424b-d511-7dc3c4bb6607"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Using cached https://files.pythonhosted.org/packages/27/3c/91ed8f5c4e7ef3227b4119200fc0ed4b4fd965b1f0172021c25701087825/transformers-3.0.2-py3-none-any.whl\n",
      "Collecting sacremoses\n",
      "  Using cached https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
      "Collecting sentencepiece!=0.1.92\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
      "\u001b[K     |████████████████████████████████| 1.1MB 3.5MB/s \n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
      "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
      "Collecting tokenizers==0.8.1.rc1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/40/d0/30d5f8d221a0ed981a186c8eb986ce1c94e3a6e87f994eae9f4aa5250217/tokenizers-0.8.1rc1-cp36-cp36m-manylinux1_x86_64.whl (3.0MB)\n",
      "\u001b[K     |████████████████████████████████| 3.0MB 7.3MB/s \n",
      "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.16.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
      "Building wheels for collected packages: sacremoses\n",
      "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893260 sha256=39ddb8812a23c09d53a93406e7272b2447289abb36d3a7188c86bfe80a02d845\n",
      "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
      "Successfully built sacremoses\n",
      "Installing collected packages: sacremoses, sentencepiece, tokenizers, transformers\n",
      "Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.8.1rc1 transformers-3.0.2\n",
      "Collecting bert-for-tf2\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/87/df/ab6d927d6162657f30eb0ae3c534c723c28c191a9caf6ee68ec935df3d0b/bert-for-tf2-0.14.5.tar.gz (40kB)\n",
      "\u001b[K     |████████████████████████████████| 40kB 2.0MB/s \n",
      "\u001b[?25hCollecting py-params>=0.9.6\n",
      "  Downloading https://files.pythonhosted.org/packages/a4/bf/c1c70d5315a8677310ea10a41cfc41c5970d9b37c31f9c90d4ab98021fd1/py-params-0.9.7.tar.gz\n",
      "Collecting params-flow>=0.8.0\n",
      "  Downloading https://files.pythonhosted.org/packages/a9/95/ff49f5ebd501f142a6f0aaf42bcfd1c192dc54909d1d9eb84ab031d46056/params-flow-0.8.2.tar.gz\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from params-flow>=0.8.0->bert-for-tf2) (1.18.5)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from params-flow>=0.8.0->bert-for-tf2) (4.41.1)\n",
      "Building wheels for collected packages: bert-for-tf2, py-params, params-flow\n",
      "  Building wheel for bert-for-tf2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for bert-for-tf2: filename=bert_for_tf2-0.14.5-cp36-none-any.whl size=30315 sha256=a85cdb2d85701b314f567e2cc33bfae29825439471262302eb226a5f9bbb7534\n",
      "  Stored in directory: /root/.cache/pip/wheels/2e/70/a2/be357037dd2cbdcaeb0add1fdf083be6a600ca65ee1f68751c\n",
      "  Building wheel for py-params (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for py-params: filename=py_params-0.9.7-cp36-none-any.whl size=7302 sha256=fb1dde212eca843e96787ca0db6c187cf42780b710dc7b5c92d70fe6f62abe3a\n",
      "  Stored in directory: /root/.cache/pip/wheels/67/f5/19/b461849a50aefdf4bab47c4756596e82ee2118b8278e5a1980\n",
      "  Building wheel for params-flow (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for params-flow: filename=params_flow-0.8.2-cp36-none-any.whl size=19473 sha256=b9bb887d6b17f0202b45273ec99f1e30493b22f7fa476cfbe5ad0ccd501b76a7\n",
      "  Stored in directory: /root/.cache/pip/wheels/08/c8/7f/81c86b9ff2b86e2c477e3914175be03e679e596067dc630c06\n",
      "Successfully built bert-for-tf2 py-params params-flow\n",
      "Installing collected packages: py-params, params-flow, bert-for-tf2\n",
      "Successfully installed bert-for-tf2-0.14.5 params-flow-0.8.2 py-params-0.9.7\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (0.1.91)\n"
     ]
    }
   ],
   "source": [
    "# Downloading required libraries for bert\n",
    "\n",
    "!pip install transformers\n",
    "!pip install bert-for-tf2\n",
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "T_btEfiep_HX"
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "import bert\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "PJzLyWUItLYI",
    "outputId": "b5d3e6b4-7230-4cce-bb16-971a64eb7405"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data</th>\n",
       "      <th>section</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hgs ihlal ge bilgi sorgula hizmet ihlal ge sor...</td>\n",
       "      <td>ekonomi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ankara sgtz bulun via twins isim kat bir kule ...</td>\n",
       "      <td>ekonomi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>akla proje ortak rus enerji dev gazprom gel iz...</td>\n",
       "      <td>ekonomi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ermenistan polis erivan merkez dn gece itibare...</td>\n",
       "      <td>ekonomi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>yldrm haziran  tarih kararname bugn resmi gaze...</td>\n",
       "      <td>ekonomi</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                data  section\n",
       "0  hgs ihlal ge bilgi sorgula hizmet ihlal ge sor...  ekonomi\n",
       "1  ankara sgtz bulun via twins isim kat bir kule ...  ekonomi\n",
       "2  akla proje ortak rus enerji dev gazprom gel iz...  ekonomi\n",
       "3  ermenistan polis erivan merkez dn gece itibare...  ekonomi\n",
       "4  yldrm haziran  tarih kararname bugn resmi gaze...  ekonomi"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('df_clean.csv')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "Zw1iq-ri0NvB",
    "outputId": "899505b1-31dd-46a8-dbf4-689fa557f02b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "kultursanat    600\n",
       "ekonomi        600\n",
       "saglık         600\n",
       "teknoloji      600\n",
       "spor           600\n",
       "siyaset        600\n",
       "Name: section, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.section.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "z5Qv1RhG0FRA",
    "outputId": "ad20ea30-e35a-4d6f-f7cb-71226f7c08e5"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data</th>\n",
       "      <th>section</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hgs ihlal ge bilgi sorgula hizmet ihlal ge sor...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ankara sgtz bulun via twins isim kat bir kule ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>akla proje ortak rus enerji dev gazprom gel iz...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ermenistan polis erivan merkez dn gece itibare...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>yldrm haziran  tarih kararname bugn resmi gaze...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                data  section\n",
       "0  hgs ihlal ge bilgi sorgula hizmet ihlal ge sor...        0\n",
       "1  ankara sgtz bulun via twins isim kat bir kule ...        0\n",
       "2  akla proje ortak rus enerji dev gazprom gel iz...        0\n",
       "3  ermenistan polis erivan merkez dn gece itibare...        0\n",
       "4  yldrm haziran  tarih kararname bugn resmi gaze...        0"
      ]
     },
     "execution_count": 9,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le = LabelEncoder()\n",
    "df.section = le.fit_transform(df.section)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U4ed2nGVE4_7"
   },
   "outputs": [],
   "source": [
    "# train-val split\n",
    "X_train, X_val, y_train, y_val = train_test_split(df.index.values, \n",
    "                                                  df.section.values, \n",
    "                                                  test_size=0.15, \n",
    "                                                  random_state=17, \n",
    "                                                  stratify=df.section.values)\n",
    "\n",
    "# with stratify parameter, We make sure that the train and val datasets contain an equal number of instances for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 452
    },
    "colab_type": "code",
    "id": "qawBhacDE6mP",
    "outputId": "f286e272-792f-47ac-aae2-9db83fd11721"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>data</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>section</th>\n",
       "      <th>data_type</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">0</th>\n",
       "      <th>train</th>\n",
       "      <td>510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>val</th>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">1</th>\n",
       "      <th>train</th>\n",
       "      <td>510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>val</th>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">2</th>\n",
       "      <th>train</th>\n",
       "      <td>510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>val</th>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">3</th>\n",
       "      <th>train</th>\n",
       "      <td>510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>val</th>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">4</th>\n",
       "      <th>train</th>\n",
       "      <td>510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>val</th>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">5</th>\n",
       "      <th>train</th>\n",
       "      <td>510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>val</th>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   data\n",
       "section data_type      \n",
       "0       train       510\n",
       "        val          90\n",
       "1       train       510\n",
       "        val          90\n",
       "2       train       510\n",
       "        val          90\n",
       "3       train       510\n",
       "        val          90\n",
       "4       train       510\n",
       "        val          90\n",
       "5       train       510\n",
       "        val          90"
      ]
     },
     "execution_count": 11,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we add a new column to data that specifies whether the row is in train or val dataset\n",
    "df['data_type'] = ['not_set'] * df.shape[0]\n",
    "df.loc[X_train, 'data_type'] = 'train'\n",
    "df.loc[X_val, 'data_type'] = 'val'\n",
    "df.groupby(['section', 'data_type']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 120,
     "referenced_widgets": [
      "3350c115b9f44a4f904e8a3fffafc177",
      "9e21a408993c49c2b8fa2cc9d42ed017",
      "fce55e5590814ecc89dad4a5028b3546",
      "06d1ab9e7eb44851be0ad47a96ce361b",
      "2824473b348741ed8c20173586323e2a",
      "66992449b82f45918dc84e5772365d06",
      "c61c3bf3df90434abef8fc4c54a55778",
      "60db2d3a78fe4f209b2a9d43619e5b6a"
     ]
    },
    "colab_type": "code",
    "id": "rYG0f83SGkTK",
    "outputId": "fe807b32-5915-477d-fde5-86f1154058ae"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3350c115b9f44a4f904e8a3fffafc177",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    }
   ],
   "source": [
    "# reading bert tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', \n",
    "                                          do_lower_case=True)\n",
    "\n",
    "# we encode train and val datasets text data with BertTokenizer's batch_encode_plus method\n",
    "encoded_data_train = tokenizer.batch_encode_plus(\n",
    "    df[df.data_type=='train'].data.values, \n",
    "    add_special_tokens=True, \n",
    "    return_attention_mask=True, \n",
    "    pad_to_max_length=True, \n",
    "    max_length=256, \n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "encoded_data_val = tokenizer.batch_encode_plus(\n",
    "    df[df.data_type=='val'].data.values, \n",
    "    add_special_tokens=True, \n",
    "    return_attention_mask=True, \n",
    "    pad_to_max_length=True, \n",
    "    max_length=256, \n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "# before creating TensorDatasets, we adapt the inputs to the BERT format\n",
    "# the inputs should have 3 columns => 'input_ids', 'attention_masks' and 'labels'\n",
    "# input_ids and attention_masks can be reachable from encoded data\n",
    "input_ids_train = encoded_data_train['input_ids']\n",
    "attention_masks_train = encoded_data_train['attention_mask']\n",
    "labels_train = torch.tensor(df[df.data_type=='train'].section.values)\n",
    "\n",
    "input_ids_val = encoded_data_val['input_ids']\n",
    "attention_masks_val = encoded_data_val['attention_mask']\n",
    "labels_val = torch.tensor(df[df.data_type=='val'].section.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "RLFKmGUXIAVz",
    "outputId": "c22646a2-eb0a-40d1-e9b4-85f5338710ca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3060\n",
      "540\n"
     ]
    }
   ],
   "source": [
    "# we create TensorDatasets for train and val data in BERT format\n",
    "dataset_train = TensorDataset(input_ids_train, attention_masks_train, labels_train)\n",
    "dataset_val = TensorDataset(input_ids_val, attention_masks_val, labels_val)\n",
    "\n",
    "\n",
    "print(len(dataset_train))\n",
    "print(len(dataset_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 220,
     "referenced_widgets": [
      "1d25fd929cdf4032a7707c78a482b77d",
      "b814ccd11ed442eda2d7781658fb83b0",
      "7432beddb0ab455fa488a40bf12fed60",
      "3c0fe3a359c64eeead9b65045853c3ab",
      "ebd9fa34107647bd9b73dfbc3b6b23b4",
      "77850034fdb04f11a1d354ad7d21ca81",
      "645e3a8e8f244318aeab28394143d2eb",
      "c5be857ac28943b3ab04a4c0a477c73b",
      "541b3d0ba5cf4091949882d30d67ea81",
      "80fa27b4006b425fb5b7c3cd001561c9",
      "1bf37b95345a49a19f308c2343d8f8f4",
      "3f37158ee16e489fb12c93e0608f7bb0",
      "84493f2ed8e94a11bc5f543afbb297ef",
      "be0b6b6794704b43979a4233d09dde4f",
      "c91ff836838143d683142731ee0da983",
      "68550ac06d354677803635d6a8160e91"
     ]
    },
    "colab_type": "code",
    "id": "-PPMGOIXI8TR",
    "outputId": "d1d84a5b-60dd-49e9-b5a9-02694b4614c4"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d25fd929cdf4032a7707c78a482b77d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=433.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "541b3d0ba5cf4091949882d30d67ea81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=440473133.0, style=ProgressStyle(descri…"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# reading the BERT model\n",
    "# num_labels parameter is the number of outputs that neural network has\n",
    "# in other words it is the number of our label classes\n",
    "# we can reach this value with label encoder object\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\",\n",
    "                                                      num_labels=len(le.classes_),\n",
    "                                                      output_attentions=False,\n",
    "                                                      output_hidden_states=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B8IToT9sI-vg"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "# we use RandomSampler for getting random samples while training the model (it is for train dataset)\n",
    "# while testing our model, it is not important whether the inputs are random or not, so we use SequentialSampler for val dataset\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "# by using TensorDataset, we create dataloader for train and val datasets\n",
    "dataloader_train = DataLoader(dataset_train, \n",
    "                              sampler=RandomSampler(dataset_train), \n",
    "                              batch_size=batch_size)\n",
    "\n",
    "dataloader_validation = DataLoader(dataset_val, \n",
    "                                   sampler=SequentialSampler(dataset_val), \n",
    "                                   batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sEJdGGansZxP"
   },
   "outputs": [],
   "source": [
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "\n",
    "# we use AdamW optimizer\n",
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr=1e-5, \n",
    "                  eps=1e-8)\n",
    "\n",
    "# set the epoch value\n",
    "epochs = 5\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps=0,\n",
    "                                            num_training_steps=len(dataloader_train)*epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X_oVX_Vot1Bq"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def f1_score_func(preds, labels):\n",
    "\n",
    "    \"\"\"\n",
    "    This method is for calculating one of the success metrics weighted f1 score.\n",
    "\n",
    "    Args:\n",
    "        preds (numpy.ndarray): predicted label values (class)\n",
    "        labels (numpy.ndarray): true label value (class)\n",
    "\n",
    "    Returns:\n",
    "        f1_score (float): f1 score value\n",
    "    \"\"\"\n",
    "\n",
    "    preds_flat = np.argmax(preds, axis=1).flatten() # with flatten(), we make a 2-D list into a 1-D list (list[list] ---flatten>>> list[])\n",
    "    labels_flat = labels.flatten()\n",
    "    return f1_score(labels_flat, preds_flat, average='weighted')\n",
    "\n",
    "# we create a dictionary that keeps class name with its label encoded value\n",
    "# example: {'ekonomi': 0, 'kultursanat': 1 ...}\n",
    "label_dict = dict(zip(le.classes_, le.transform(le.classes_)))\n",
    "\n",
    "def accuracy_per_class(preds, labels):\n",
    "    \n",
    "    \"\"\"\n",
    "    This method is for calculating one of the success metrics accuracy.\n",
    "    It calculates accuracy value for each classes and also total accuracy.\n",
    "    And it prints each values.\n",
    "\n",
    "    Args:\n",
    "        preds (numpy.ndarray): predicted label values (class)\n",
    "        labels (numpy.ndarray): true label value (class)\n",
    "\n",
    "    Returns: None\n",
    "    \"\"\"\n",
    "\n",
    "    label_dict_inverse = {v: k for k, v in label_dict.items()}\n",
    "    \n",
    "    preds_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "\n",
    "    # total accuracy = count of total true predictions / count of total predictions\n",
    "    total_pred_positive = 0 # to calculate total accuracy, we keep the total count of prediction that is correct\n",
    "    total = 0 # also, we keep the count of total predictions\n",
    "\n",
    "    print('\\n')\n",
    "\n",
    "    for label in np.unique(labels_flat):\n",
    "        y_preds = preds_flat[labels_flat==label]\n",
    "        y_true = labels_flat[labels_flat==label]\n",
    "        print(f'Class: {label_dict_inverse[label]}')\n",
    "        print(f'Accuracy: {len(y_preds[y_preds==label])}/{len(y_true)}\\n')\n",
    "\n",
    "        total_pred_positive += len(y_preds[y_preds==label])\n",
    "        total += len(y_true)\n",
    "\n",
    "    print('Total accuracy: ', total_pred_positive/total)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "U9DX_nHMsZmP",
    "outputId": "d2f9cf09-4565-4b1c-9877-094c6fd12726"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "seed_val = 17\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "# we define our device as cuda because we use gpu \n",
    "# if you are using cpu instead, you need to change 'cuda' to 'cpu'\n",
    "device = torch.device('cuda') \n",
    "model.to(device) # we send our model to device (cuda)\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rT8IoLHosYoQ"
   },
   "outputs": [],
   "source": [
    "def evaluate(dataloader_val):\n",
    "\n",
    "    \"\"\"\n",
    "    This method is for testing the model which we trained with validation data.\n",
    "    It produce predictions and calculates the loss value.\n",
    "\n",
    "    Args:\n",
    "        dataloader_val (torch.utils.data.dataloader.DataLoader): dataloader for validation dataset\n",
    "\n",
    "    Returns:\n",
    "        loss_val_avg (float): average loss value of validation dataset\n",
    "        predictions (numpy.ndarray): predicted label values (class)\n",
    "        true_vals (numpy.ndarray): true label value (class)\n",
    "    \"\"\"\n",
    "\n",
    "    model.eval() # we set our model to eval mode. (this is the mode that we test our model with val dataset)\n",
    "    \n",
    "    loss_val_total = 0\n",
    "    predictions, true_vals = [], []\n",
    "    \n",
    "    for batch in tqdm(dataloader_val):\n",
    "        \n",
    "        batch = tuple(b.to(device) for b in batch)\n",
    "        \n",
    "        # BERT's input format that I was mantioned above\n",
    "        inputs = {'input_ids':      batch[0],\n",
    "                  'attention_mask': batch[1],\n",
    "                  'labels':         batch[2],\n",
    "                 }\n",
    "\n",
    "        with torch.no_grad():        \n",
    "            outputs = model(**inputs)\n",
    "            \n",
    "        loss = outputs[0] # loss value\n",
    "        logits = outputs[1]\n",
    "        loss_val_total += loss.item()\n",
    "\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = inputs['labels'].cpu().numpy()\n",
    "        predictions.append(logits)\n",
    "        true_vals.append(label_ids)\n",
    "    \n",
    "    loss_val_avg = loss_val_total/len(dataloader_val) \n",
    "    \n",
    "    predictions = np.concatenate(predictions, axis=0)\n",
    "    true_vals = np.concatenate(true_vals, axis=0)\n",
    "            \n",
    "    return loss_val_avg, predictions, true_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "ECqc1t4QsYCr",
    "outputId": "90a0d576-05e6-4aeb-e67d-8daf1c3943ae",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]\n",
      "Epoch 1:   0%|          | 0/96 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 1:   0%|          | 0/96 [00:02<?, ?it/s, training_loss=0.638]\u001b[A\n",
      "Epoch 1:   1%|          | 1/96 [00:02<04:26,  2.80s/it, training_loss=0.638]\u001b[A\n",
      "Epoch 1:   1%|          | 1/96 [00:05<04:26,  2.80s/it, training_loss=0.579]\u001b[A\n",
      "Epoch 1:   2%|▏         | 2/96 [00:05<04:13,  2.69s/it, training_loss=0.579]\u001b[A\n",
      "Epoch 1:   2%|▏         | 2/96 [00:07<04:13,  2.69s/it, training_loss=0.598]\u001b[A\n",
      "Epoch 1:   3%|▎         | 3/96 [00:07<04:02,  2.61s/it, training_loss=0.598]\u001b[A\n",
      "Epoch 1:   3%|▎         | 3/96 [00:10<04:02,  2.61s/it, training_loss=0.597]\u001b[A\n",
      "Epoch 1:   4%|▍         | 4/96 [00:10<03:55,  2.56s/it, training_loss=0.597]\u001b[A\n",
      "Epoch 1:   4%|▍         | 4/96 [00:12<03:55,  2.56s/it, training_loss=0.575]\u001b[A\n",
      "Epoch 1:   5%|▌         | 5/96 [00:12<03:49,  2.52s/it, training_loss=0.575]\u001b[A\n",
      "Epoch 1:   5%|▌         | 5/96 [00:14<03:49,  2.52s/it, training_loss=0.588]\u001b[A\n",
      "Epoch 1:   6%|▋         | 6/96 [00:14<03:45,  2.50s/it, training_loss=0.588]\u001b[A\n",
      "Epoch 1:   6%|▋         | 6/96 [00:17<03:45,  2.50s/it, training_loss=0.601]\u001b[A\n",
      "Epoch 1:   7%|▋         | 7/96 [00:17<03:40,  2.48s/it, training_loss=0.601]\u001b[A\n",
      "Epoch 1:   7%|▋         | 7/96 [00:19<03:40,  2.48s/it, training_loss=0.598]\u001b[A\n",
      "Epoch 1:   8%|▊         | 8/96 [00:19<03:37,  2.47s/it, training_loss=0.598]\u001b[A\n",
      "Epoch 1:   8%|▊         | 8/96 [00:22<03:37,  2.47s/it, training_loss=0.599]\u001b[A\n",
      "Epoch 1:   9%|▉         | 9/96 [00:22<03:34,  2.46s/it, training_loss=0.599]\u001b[A\n",
      "Epoch 1:   9%|▉         | 9/96 [00:24<03:34,  2.46s/it, training_loss=0.590]\u001b[A\n",
      "Epoch 1:  10%|█         | 10/96 [00:24<03:31,  2.45s/it, training_loss=0.590]\u001b[A\n",
      "Epoch 1:  10%|█         | 10/96 [00:27<03:31,  2.45s/it, training_loss=0.622]\u001b[A\n",
      "Epoch 1:  11%|█▏        | 11/96 [00:27<03:28,  2.45s/it, training_loss=0.622]\u001b[A\n",
      "Epoch 1:  11%|█▏        | 11/96 [00:29<03:28,  2.45s/it, training_loss=0.597]\u001b[A\n",
      "Epoch 1:  12%|█▎        | 12/96 [00:29<03:26,  2.45s/it, training_loss=0.597]\u001b[A\n",
      "Epoch 1:  12%|█▎        | 12/96 [00:32<03:26,  2.45s/it, training_loss=0.589]\u001b[A\n",
      "Epoch 1:  14%|█▎        | 13/96 [00:32<03:23,  2.45s/it, training_loss=0.589]\u001b[A\n",
      "Epoch 1:  14%|█▎        | 13/96 [00:34<03:23,  2.45s/it, training_loss=0.596]\u001b[A\n",
      "Epoch 1:  15%|█▍        | 14/96 [00:34<03:21,  2.45s/it, training_loss=0.596]\u001b[A\n",
      "Epoch 1:  15%|█▍        | 14/96 [00:37<03:21,  2.45s/it, training_loss=0.572]\u001b[A\n",
      "Epoch 1:  16%|█▌        | 15/96 [00:37<03:19,  2.46s/it, training_loss=0.572]\u001b[A\n",
      "Epoch 1:  16%|█▌        | 15/96 [00:39<03:19,  2.46s/it, training_loss=0.580]\u001b[A\n",
      "Epoch 1:  17%|█▋        | 16/96 [00:39<03:16,  2.46s/it, training_loss=0.580]\u001b[A\n",
      "Epoch 1:  17%|█▋        | 16/96 [00:41<03:16,  2.46s/it, training_loss=0.558]\u001b[A\n",
      "Epoch 1:  18%|█▊        | 17/96 [00:41<03:14,  2.46s/it, training_loss=0.558]\u001b[A\n",
      "Epoch 1:  18%|█▊        | 17/96 [00:44<03:14,  2.46s/it, training_loss=0.570]\u001b[A\n",
      "Epoch 1:  19%|█▉        | 18/96 [00:44<03:11,  2.45s/it, training_loss=0.570]\u001b[A\n",
      "Epoch 1:  19%|█▉        | 18/96 [00:46<03:11,  2.45s/it, training_loss=0.576]\u001b[A\n",
      "Epoch 1:  20%|█▉        | 19/96 [00:46<03:08,  2.45s/it, training_loss=0.576]\u001b[A\n",
      "Epoch 1:  20%|█▉        | 19/96 [00:49<03:08,  2.45s/it, training_loss=0.591]\u001b[A\n",
      "Epoch 1:  21%|██        | 20/96 [00:49<03:06,  2.45s/it, training_loss=0.591]\u001b[A\n",
      "Epoch 1:  21%|██        | 20/96 [00:51<03:06,  2.45s/it, training_loss=0.557]\u001b[A\n",
      "Epoch 1:  22%|██▏       | 21/96 [00:51<03:04,  2.46s/it, training_loss=0.557]\u001b[A\n",
      "Epoch 1:  22%|██▏       | 21/96 [00:54<03:04,  2.46s/it, training_loss=0.552]\u001b[A\n",
      "Epoch 1:  23%|██▎       | 22/96 [00:54<03:01,  2.46s/it, training_loss=0.552]\u001b[A\n",
      "Epoch 1:  23%|██▎       | 22/96 [00:56<03:01,  2.46s/it, training_loss=0.562]\u001b[A\n",
      "Epoch 1:  24%|██▍       | 23/96 [00:56<02:59,  2.46s/it, training_loss=0.562]\u001b[A\n",
      "Epoch 1:  24%|██▍       | 23/96 [00:59<02:59,  2.46s/it, training_loss=0.549]\u001b[A\n",
      "Epoch 1:  25%|██▌       | 24/96 [00:59<02:57,  2.46s/it, training_loss=0.549]\u001b[A\n",
      "Epoch 1:  25%|██▌       | 24/96 [01:01<02:57,  2.46s/it, training_loss=0.555]\u001b[A\n",
      "Epoch 1:  26%|██▌       | 25/96 [01:01<02:54,  2.46s/it, training_loss=0.555]\u001b[A\n",
      "Epoch 1:  26%|██▌       | 25/96 [01:04<02:54,  2.46s/it, training_loss=0.544]\u001b[A\n",
      "Epoch 1:  27%|██▋       | 26/96 [01:04<02:52,  2.46s/it, training_loss=0.544]\u001b[A\n",
      "Epoch 1:  27%|██▋       | 26/96 [01:06<02:52,  2.46s/it, training_loss=0.533]\u001b[A\n",
      "Epoch 1:  28%|██▊       | 27/96 [01:06<02:50,  2.46s/it, training_loss=0.533]\u001b[A\n",
      "Epoch 1:  28%|██▊       | 27/96 [01:08<02:50,  2.46s/it, training_loss=0.519]\u001b[A\n",
      "Epoch 1:  29%|██▉       | 28/96 [01:09<02:47,  2.47s/it, training_loss=0.519]\u001b[A\n",
      "Epoch 1:  29%|██▉       | 28/96 [01:11<02:47,  2.47s/it, training_loss=0.542]\u001b[A\n",
      "Epoch 1:  30%|███       | 29/96 [01:11<02:46,  2.48s/it, training_loss=0.542]\u001b[A\n",
      "Epoch 1:  30%|███       | 29/96 [01:13<02:46,  2.48s/it, training_loss=0.554]\u001b[A\n",
      "Epoch 1:  31%|███▏      | 30/96 [01:13<02:43,  2.48s/it, training_loss=0.554]\u001b[A\n",
      "Epoch 1:  31%|███▏      | 30/96 [01:16<02:43,  2.48s/it, training_loss=0.537]\u001b[A\n",
      "Epoch 1:  32%|███▏      | 31/96 [01:16<02:41,  2.49s/it, training_loss=0.537]\u001b[A\n",
      "Epoch 1:  32%|███▏      | 31/96 [01:18<02:41,  2.49s/it, training_loss=0.513]\u001b[A\n",
      "Epoch 1:  33%|███▎      | 32/96 [01:19<02:39,  2.49s/it, training_loss=0.513]\u001b[A\n",
      "Epoch 1:  33%|███▎      | 32/96 [01:21<02:39,  2.49s/it, training_loss=0.536]\u001b[A\n",
      "Epoch 1:  34%|███▍      | 33/96 [01:21<02:37,  2.50s/it, training_loss=0.536]\u001b[A\n",
      "Epoch 1:  34%|███▍      | 33/96 [01:24<02:37,  2.50s/it, training_loss=0.504]\u001b[A\n",
      "Epoch 1:  35%|███▌      | 34/96 [01:24<02:35,  2.50s/it, training_loss=0.504]\u001b[A\n",
      "Epoch 1:  35%|███▌      | 34/96 [01:26<02:35,  2.50s/it, training_loss=0.497]\u001b[A\n",
      "Epoch 1:  36%|███▋      | 35/96 [01:26<02:32,  2.51s/it, training_loss=0.497]\u001b[A\n",
      "Epoch 1:  36%|███▋      | 35/96 [01:29<02:32,  2.51s/it, training_loss=0.486]\u001b[A\n",
      "Epoch 1:  38%|███▊      | 36/96 [01:29<02:30,  2.51s/it, training_loss=0.486]\u001b[A\n",
      "Epoch 1:  38%|███▊      | 36/96 [01:31<02:30,  2.51s/it, training_loss=0.521]\u001b[A\n",
      "Epoch 1:  39%|███▊      | 37/96 [01:31<02:27,  2.51s/it, training_loss=0.521]\u001b[A\n",
      "Epoch 1:  39%|███▊      | 37/96 [01:34<02:27,  2.51s/it, training_loss=0.479]\u001b[A\n",
      "Epoch 1:  40%|███▉      | 38/96 [01:34<02:25,  2.51s/it, training_loss=0.479]\u001b[A\n",
      "Epoch 1:  40%|███▉      | 38/96 [01:36<02:25,  2.51s/it, training_loss=0.484]\u001b[A\n",
      "Epoch 1:  41%|████      | 39/96 [01:36<02:23,  2.51s/it, training_loss=0.484]\u001b[A\n",
      "Epoch 1:  41%|████      | 39/96 [01:39<02:23,  2.51s/it, training_loss=0.480]\u001b[A\n",
      "Epoch 1:  42%|████▏     | 40/96 [01:39<02:20,  2.51s/it, training_loss=0.480]\u001b[A\n",
      "Epoch 1:  42%|████▏     | 40/96 [01:41<02:20,  2.51s/it, training_loss=0.489]\u001b[A\n",
      "Epoch 1:  43%|████▎     | 41/96 [01:41<02:18,  2.51s/it, training_loss=0.489]\u001b[A\n",
      "Epoch 1:  43%|████▎     | 41/96 [01:44<02:18,  2.51s/it, training_loss=0.438]\u001b[A\n",
      "Epoch 1:  44%|████▍     | 42/96 [01:44<02:15,  2.51s/it, training_loss=0.438]\u001b[A\n",
      "Epoch 1:  44%|████▍     | 42/96 [01:46<02:15,  2.51s/it, training_loss=0.464]\u001b[A\n",
      "Epoch 1:  45%|████▍     | 43/96 [01:46<02:13,  2.52s/it, training_loss=0.464]\u001b[A\n",
      "Epoch 1:  45%|████▍     | 43/96 [01:49<02:13,  2.52s/it, training_loss=0.491]\u001b[A\n",
      "Epoch 1:  46%|████▌     | 44/96 [01:49<02:10,  2.52s/it, training_loss=0.491]\u001b[A\n",
      "Epoch 1:  46%|████▌     | 44/96 [01:51<02:10,  2.52s/it, training_loss=0.483]\u001b[A\n",
      "Epoch 1:  47%|████▋     | 45/96 [01:51<02:08,  2.52s/it, training_loss=0.483]\u001b[A\n",
      "Epoch 1:  47%|████▋     | 45/96 [01:54<02:08,  2.52s/it, training_loss=0.472]\u001b[A\n",
      "Epoch 1:  48%|████▊     | 46/96 [01:54<02:05,  2.51s/it, training_loss=0.472]\u001b[A\n",
      "Epoch 1:  48%|████▊     | 46/96 [01:56<02:05,  2.51s/it, training_loss=0.452]\u001b[A\n",
      "Epoch 1:  49%|████▉     | 47/96 [01:56<02:03,  2.51s/it, training_loss=0.452]\u001b[A\n",
      "Epoch 1:  49%|████▉     | 47/96 [01:59<02:03,  2.51s/it, training_loss=0.461]\u001b[A\n",
      "Epoch 1:  50%|█████     | 48/96 [01:59<02:00,  2.51s/it, training_loss=0.461]\u001b[A\n",
      "Epoch 1:  50%|█████     | 48/96 [02:01<02:00,  2.51s/it, training_loss=0.490]\u001b[A\n",
      "Epoch 1:  51%|█████     | 49/96 [02:01<01:58,  2.52s/it, training_loss=0.490]\u001b[A\n",
      "Epoch 1:  51%|█████     | 49/96 [02:04<01:58,  2.52s/it, training_loss=0.470]\u001b[A\n",
      "Epoch 1:  52%|█████▏    | 50/96 [02:04<01:55,  2.52s/it, training_loss=0.470]\u001b[A\n",
      "Epoch 1:  52%|█████▏    | 50/96 [02:06<01:55,  2.52s/it, training_loss=0.553]\u001b[A\n",
      "Epoch 1:  53%|█████▎    | 51/96 [02:06<01:53,  2.51s/it, training_loss=0.553]\u001b[A\n",
      "Epoch 1:  53%|█████▎    | 51/96 [02:09<01:53,  2.51s/it, training_loss=0.470]\u001b[A\n",
      "Epoch 1:  54%|█████▍    | 52/96 [02:09<01:50,  2.51s/it, training_loss=0.470]\u001b[A\n",
      "Epoch 1:  54%|█████▍    | 52/96 [02:11<01:50,  2.51s/it, training_loss=0.470]\u001b[A\n",
      "Epoch 1:  55%|█████▌    | 53/96 [02:11<01:47,  2.51s/it, training_loss=0.470]\u001b[A\n",
      "Epoch 1:  55%|█████▌    | 53/96 [02:14<01:47,  2.51s/it, training_loss=0.438]\u001b[A\n",
      "Epoch 1:  56%|█████▋    | 54/96 [02:14<01:45,  2.51s/it, training_loss=0.438]\u001b[A\n",
      "Epoch 1:  56%|█████▋    | 54/96 [02:16<01:45,  2.51s/it, training_loss=0.440]\u001b[A\n",
      "Epoch 1:  57%|█████▋    | 55/96 [02:16<01:42,  2.51s/it, training_loss=0.440]\u001b[A\n",
      "Epoch 1:  57%|█████▋    | 55/96 [02:19<01:42,  2.51s/it, training_loss=0.445]\u001b[A\n",
      "Epoch 1:  58%|█████▊    | 56/96 [02:19<01:40,  2.51s/it, training_loss=0.445]\u001b[A\n",
      "Epoch 1:  58%|█████▊    | 56/96 [02:21<01:40,  2.51s/it, training_loss=0.485]\u001b[A\n",
      "Epoch 1:  59%|█████▉    | 57/96 [02:21<01:37,  2.51s/it, training_loss=0.485]\u001b[A\n",
      "Epoch 1:  59%|█████▉    | 57/96 [02:24<01:37,  2.51s/it, training_loss=0.405]\u001b[A\n",
      "Epoch 1:  60%|██████    | 58/96 [02:24<01:35,  2.51s/it, training_loss=0.405]\u001b[A\n",
      "Epoch 1:  60%|██████    | 58/96 [02:26<01:35,  2.51s/it, training_loss=0.500]\u001b[A\n",
      "Epoch 1:  61%|██████▏   | 59/96 [02:26<01:32,  2.51s/it, training_loss=0.500]\u001b[A\n",
      "Epoch 1:  61%|██████▏   | 59/96 [02:29<01:32,  2.51s/it, training_loss=0.419]\u001b[A\n",
      "Epoch 1:  62%|██████▎   | 60/96 [02:29<01:30,  2.52s/it, training_loss=0.419]\u001b[A\n",
      "Epoch 1:  62%|██████▎   | 60/96 [02:31<01:30,  2.52s/it, training_loss=0.386]\u001b[A\n",
      "Epoch 1:  64%|██████▎   | 61/96 [02:31<01:28,  2.51s/it, training_loss=0.386]\u001b[A\n",
      "Epoch 1:  64%|██████▎   | 61/96 [02:34<01:28,  2.51s/it, training_loss=0.422]\u001b[A\n",
      "Epoch 1:  65%|██████▍   | 62/96 [02:34<01:25,  2.52s/it, training_loss=0.422]\u001b[A\n",
      "Epoch 1:  65%|██████▍   | 62/96 [02:36<01:25,  2.52s/it, training_loss=0.414]\u001b[A\n",
      "Epoch 1:  66%|██████▌   | 63/96 [02:36<01:23,  2.52s/it, training_loss=0.414]\u001b[A\n",
      "Epoch 1:  66%|██████▌   | 63/96 [02:39<01:23,  2.52s/it, training_loss=0.416]\u001b[A\n",
      "Epoch 1:  67%|██████▋   | 64/96 [02:39<01:20,  2.52s/it, training_loss=0.416]\u001b[A\n",
      "Epoch 1:  67%|██████▋   | 64/96 [02:41<01:20,  2.52s/it, training_loss=0.443]\u001b[A\n",
      "Epoch 1:  68%|██████▊   | 65/96 [02:41<01:18,  2.52s/it, training_loss=0.443]\u001b[A\n",
      "Epoch 1:  68%|██████▊   | 65/96 [02:44<01:18,  2.52s/it, training_loss=0.489]\u001b[A\n",
      "Epoch 1:  69%|██████▉   | 66/96 [02:44<01:15,  2.52s/it, training_loss=0.489]\u001b[A\n",
      "Epoch 1:  69%|██████▉   | 66/96 [02:47<01:15,  2.52s/it, training_loss=0.424]\u001b[A\n",
      "Epoch 1:  70%|██████▉   | 67/96 [02:47<01:12,  2.52s/it, training_loss=0.424]\u001b[A\n",
      "Epoch 1:  70%|██████▉   | 67/96 [02:49<01:12,  2.52s/it, training_loss=0.403]\u001b[A\n",
      "Epoch 1:  71%|███████   | 68/96 [02:49<01:10,  2.51s/it, training_loss=0.403]\u001b[A\n",
      "Epoch 1:  71%|███████   | 68/96 [02:52<01:10,  2.51s/it, training_loss=0.365]\u001b[A\n",
      "Epoch 1:  72%|███████▏  | 69/96 [02:52<01:07,  2.51s/it, training_loss=0.365]\u001b[A\n",
      "Epoch 1:  72%|███████▏  | 69/96 [02:54<01:07,  2.51s/it, training_loss=0.367]\u001b[A\n",
      "Epoch 1:  73%|███████▎  | 70/96 [02:54<01:05,  2.52s/it, training_loss=0.367]\u001b[A\n",
      "Epoch 1:  73%|███████▎  | 70/96 [02:57<01:05,  2.52s/it, training_loss=0.392]\u001b[A\n",
      "Epoch 1:  74%|███████▍  | 71/96 [02:57<01:02,  2.51s/it, training_loss=0.392]\u001b[A\n",
      "Epoch 1:  74%|███████▍  | 71/96 [02:59<01:02,  2.51s/it, training_loss=0.390]\u001b[A\n",
      "Epoch 1:  75%|███████▌  | 72/96 [02:59<01:00,  2.51s/it, training_loss=0.390]\u001b[A\n",
      "Epoch 1:  75%|███████▌  | 72/96 [03:02<01:00,  2.51s/it, training_loss=0.383]\u001b[A\n",
      "Epoch 1:  76%|███████▌  | 73/96 [03:02<00:57,  2.51s/it, training_loss=0.383]\u001b[A\n",
      "Epoch 1:  76%|███████▌  | 73/96 [03:04<00:57,  2.51s/it, training_loss=0.418]\u001b[A\n",
      "Epoch 1:  77%|███████▋  | 74/96 [03:04<00:55,  2.51s/it, training_loss=0.418]\u001b[A\n",
      "Epoch 1:  77%|███████▋  | 74/96 [03:07<00:55,  2.51s/it, training_loss=0.442]\u001b[A\n",
      "Epoch 1:  78%|███████▊  | 75/96 [03:07<00:52,  2.51s/it, training_loss=0.442]\u001b[A\n",
      "Epoch 1:  78%|███████▊  | 75/96 [03:09<00:52,  2.51s/it, training_loss=0.383]\u001b[A\n",
      "Epoch 1:  79%|███████▉  | 76/96 [03:09<00:50,  2.51s/it, training_loss=0.383]\u001b[A\n",
      "Epoch 1:  79%|███████▉  | 76/96 [03:12<00:50,  2.51s/it, training_loss=0.339]\u001b[A\n",
      "Epoch 1:  80%|████████  | 77/96 [03:12<00:47,  2.51s/it, training_loss=0.339]\u001b[A\n",
      "Epoch 1:  80%|████████  | 77/96 [03:14<00:47,  2.51s/it, training_loss=0.389]\u001b[A\n",
      "Epoch 1:  81%|████████▏ | 78/96 [03:14<00:45,  2.51s/it, training_loss=0.389]\u001b[A\n",
      "Epoch 1:  81%|████████▏ | 78/96 [03:17<00:45,  2.51s/it, training_loss=0.435]\u001b[A\n",
      "Epoch 1:  82%|████████▏ | 79/96 [03:17<00:42,  2.51s/it, training_loss=0.435]\u001b[A\n",
      "Epoch 1:  82%|████████▏ | 79/96 [03:19<00:42,  2.51s/it, training_loss=0.375]\u001b[A\n",
      "Epoch 1:  83%|████████▎ | 80/96 [03:19<00:40,  2.51s/it, training_loss=0.375]\u001b[A\n",
      "Epoch 1:  83%|████████▎ | 80/96 [03:22<00:40,  2.51s/it, training_loss=0.373]\u001b[A\n",
      "Epoch 1:  84%|████████▍ | 81/96 [03:22<00:37,  2.52s/it, training_loss=0.373]\u001b[A\n",
      "Epoch 1:  84%|████████▍ | 81/96 [03:24<00:37,  2.52s/it, training_loss=0.388]\u001b[A\n",
      "Epoch 1:  85%|████████▌ | 82/96 [03:24<00:35,  2.52s/it, training_loss=0.388]\u001b[A\n",
      "Epoch 1:  85%|████████▌ | 82/96 [03:27<00:35,  2.52s/it, training_loss=0.319]\u001b[A\n",
      "Epoch 1:  86%|████████▋ | 83/96 [03:27<00:32,  2.52s/it, training_loss=0.319]\u001b[A\n",
      "Epoch 1:  86%|████████▋ | 83/96 [03:29<00:32,  2.52s/it, training_loss=0.356]\u001b[A\n",
      "Epoch 1:  88%|████████▊ | 84/96 [03:29<00:30,  2.52s/it, training_loss=0.356]\u001b[A\n",
      "Epoch 1:  88%|████████▊ | 84/96 [03:32<00:30,  2.52s/it, training_loss=0.361]\u001b[A\n",
      "Epoch 1:  89%|████████▊ | 85/96 [03:32<00:27,  2.52s/it, training_loss=0.361]\u001b[A\n",
      "Epoch 1:  89%|████████▊ | 85/96 [03:34<00:27,  2.52s/it, training_loss=0.396]\u001b[A\n",
      "Epoch 1:  90%|████████▉ | 86/96 [03:34<00:25,  2.52s/it, training_loss=0.396]\u001b[A\n",
      "Epoch 1:  90%|████████▉ | 86/96 [03:37<00:25,  2.52s/it, training_loss=0.329]\u001b[A\n",
      "Epoch 1:  91%|█████████ | 87/96 [03:37<00:22,  2.52s/it, training_loss=0.329]\u001b[A\n",
      "Epoch 1:  91%|█████████ | 87/96 [03:39<00:22,  2.52s/it, training_loss=0.400]\u001b[A\n",
      "Epoch 1:  92%|█████████▏| 88/96 [03:39<00:20,  2.52s/it, training_loss=0.400]\u001b[A\n",
      "Epoch 1:  92%|█████████▏| 88/96 [03:42<00:20,  2.52s/it, training_loss=0.412]\u001b[A\n",
      "Epoch 1:  93%|█████████▎| 89/96 [03:42<00:17,  2.52s/it, training_loss=0.412]\u001b[A\n",
      "Epoch 1:  93%|█████████▎| 89/96 [03:44<00:17,  2.52s/it, training_loss=0.381]\u001b[A\n",
      "Epoch 1:  94%|█████████▍| 90/96 [03:44<00:15,  2.52s/it, training_loss=0.381]\u001b[A\n",
      "Epoch 1:  94%|█████████▍| 90/96 [03:47<00:15,  2.52s/it, training_loss=0.350]\u001b[A\n",
      "Epoch 1:  95%|█████████▍| 91/96 [03:47<00:12,  2.52s/it, training_loss=0.350]\u001b[A\n",
      "Epoch 1:  95%|█████████▍| 91/96 [03:49<00:12,  2.52s/it, training_loss=0.358]\u001b[A\n",
      "Epoch 1:  96%|█████████▌| 92/96 [03:49<00:10,  2.51s/it, training_loss=0.358]\u001b[A\n",
      "Epoch 1:  96%|█████████▌| 92/96 [03:52<00:10,  2.51s/it, training_loss=0.332]\u001b[A\n",
      "Epoch 1:  97%|█████████▋| 93/96 [03:52<00:07,  2.51s/it, training_loss=0.332]\u001b[A\n",
      "Epoch 1:  97%|█████████▋| 93/96 [03:54<00:07,  2.51s/it, training_loss=0.337]\u001b[A\n",
      "Epoch 1:  98%|█████████▊| 94/96 [03:54<00:05,  2.51s/it, training_loss=0.337]\u001b[A\n",
      "Epoch 1:  98%|█████████▊| 94/96 [03:57<00:05,  2.51s/it, training_loss=0.402]\u001b[A\n",
      "Epoch 1:  99%|█████████▉| 95/96 [03:57<00:02,  2.51s/it, training_loss=0.402]\u001b[A\n",
      "Epoch 1:  99%|█████████▉| 95/96 [03:59<00:02,  2.51s/it, training_loss=0.352]\u001b[A\n",
      "Epoch 1: 100%|██████████| 96/96 [03:59<00:00,  2.24s/it, training_loss=0.352]\u001b[A\n",
      "  0%|          | 0/5 [04:00<?, ?it/s]\n",
      "  0%|          | 0/17 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1\n",
      "Training loss: 1.4242615662515163\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  6%|▌         | 1/17 [00:00<00:14,  1.11it/s]\u001b[A\n",
      " 12%|█▏        | 2/17 [00:01<00:13,  1.10it/s]\u001b[A\n",
      " 18%|█▊        | 3/17 [00:02<00:12,  1.09it/s]\u001b[A\n",
      " 24%|██▎       | 4/17 [00:03<00:11,  1.09it/s]\u001b[A\n",
      " 29%|██▉       | 5/17 [00:04<00:10,  1.09it/s]\u001b[A\n",
      " 35%|███▌      | 6/17 [00:05<00:10,  1.09it/s]\u001b[A\n",
      " 41%|████      | 7/17 [00:06<00:09,  1.09it/s]\u001b[A\n",
      " 47%|████▋     | 8/17 [00:07<00:08,  1.08it/s]\u001b[A\n",
      " 53%|█████▎    | 9/17 [00:08<00:07,  1.08it/s]\u001b[A\n",
      " 59%|█████▉    | 10/17 [00:09<00:06,  1.08it/s]\u001b[A\n",
      " 65%|██████▍   | 11/17 [00:10<00:05,  1.09it/s]\u001b[A\n",
      " 71%|███████   | 12/17 [00:11<00:04,  1.09it/s]\u001b[A\n",
      " 76%|███████▋  | 13/17 [00:11<00:03,  1.09it/s]\u001b[A\n",
      " 82%|████████▏ | 14/17 [00:12<00:02,  1.09it/s]\u001b[A\n",
      " 88%|████████▊ | 15/17 [00:13<00:01,  1.09it/s]\u001b[A\n",
      " 94%|█████████▍| 16/17 [00:14<00:00,  1.09it/s]\u001b[A\n",
      "100%|██████████| 17/17 [00:15<00:00,  1.09it/s]\n",
      " 20%|██        | 1/5 [04:16<17:04, 256.13s/it]\n",
      "Epoch 2:   0%|          | 0/96 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 1.0219465231194216\n",
      "F1 Score (Weighted): 0.6556919134697533\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2:   0%|          | 0/96 [00:02<?, ?it/s, training_loss=0.360]\u001b[A\n",
      "Epoch 2:   1%|          | 1/96 [00:02<03:58,  2.51s/it, training_loss=0.360]\u001b[A\n",
      "Epoch 2:   1%|          | 1/96 [00:05<03:58,  2.51s/it, training_loss=0.333]\u001b[A\n",
      "Epoch 2:   2%|▏         | 2/96 [00:05<03:56,  2.51s/it, training_loss=0.333]\u001b[A\n",
      "Epoch 2:   2%|▏         | 2/96 [00:07<03:56,  2.51s/it, training_loss=0.366]\u001b[A\n",
      "Epoch 2:   3%|▎         | 3/96 [00:07<03:53,  2.52s/it, training_loss=0.366]\u001b[A\n",
      "Epoch 2:   3%|▎         | 3/96 [00:10<03:53,  2.52s/it, training_loss=0.354]\u001b[A\n",
      "Epoch 2:   4%|▍         | 4/96 [00:10<03:51,  2.52s/it, training_loss=0.354]\u001b[A\n",
      "Epoch 2:   4%|▍         | 4/96 [00:12<03:51,  2.52s/it, training_loss=0.349]\u001b[A\n",
      "Epoch 2:   5%|▌         | 5/96 [00:12<03:49,  2.52s/it, training_loss=0.349]\u001b[A\n",
      "Epoch 2:   5%|▌         | 5/96 [00:15<03:49,  2.52s/it, training_loss=0.319]\u001b[A\n",
      "Epoch 2:   6%|▋         | 6/96 [00:15<03:46,  2.52s/it, training_loss=0.319]\u001b[A\n",
      "Epoch 2:   6%|▋         | 6/96 [00:17<03:46,  2.52s/it, training_loss=0.329]\u001b[A\n",
      "Epoch 2:   7%|▋         | 7/96 [00:17<03:43,  2.52s/it, training_loss=0.329]\u001b[A\n",
      "Epoch 2:   7%|▋         | 7/96 [00:20<03:43,  2.52s/it, training_loss=0.328]\u001b[A\n",
      "Epoch 2:   8%|▊         | 8/96 [00:20<03:41,  2.52s/it, training_loss=0.328]\u001b[A\n",
      "Epoch 2:   8%|▊         | 8/96 [00:22<03:41,  2.52s/it, training_loss=0.354]\u001b[A\n",
      "Epoch 2:   9%|▉         | 9/96 [00:22<03:38,  2.52s/it, training_loss=0.354]\u001b[A\n",
      "Epoch 2:   9%|▉         | 9/96 [00:25<03:38,  2.52s/it, training_loss=0.304]\u001b[A\n",
      "Epoch 2:  10%|█         | 10/96 [00:25<03:36,  2.51s/it, training_loss=0.304]\u001b[A\n",
      "Epoch 2:  10%|█         | 10/96 [00:27<03:36,  2.51s/it, training_loss=0.336]\u001b[A\n",
      "Epoch 2:  11%|█▏        | 11/96 [00:27<03:33,  2.51s/it, training_loss=0.336]\u001b[A\n",
      "Epoch 2:  11%|█▏        | 11/96 [00:30<03:33,  2.51s/it, training_loss=0.304]\u001b[A\n",
      "Epoch 2:  12%|█▎        | 12/96 [00:30<03:30,  2.51s/it, training_loss=0.304]\u001b[A\n",
      "Epoch 2:  12%|█▎        | 12/96 [00:32<03:30,  2.51s/it, training_loss=0.324]\u001b[A\n",
      "Epoch 2:  14%|█▎        | 13/96 [00:32<03:28,  2.51s/it, training_loss=0.324]\u001b[A\n",
      "Epoch 2:  14%|█▎        | 13/96 [00:35<03:28,  2.51s/it, training_loss=0.325]\u001b[A\n",
      "Epoch 2:  15%|█▍        | 14/96 [00:35<03:26,  2.51s/it, training_loss=0.325]\u001b[A\n",
      "Epoch 2:  15%|█▍        | 14/96 [00:37<03:26,  2.51s/it, training_loss=0.350]\u001b[A\n",
      "Epoch 2:  16%|█▌        | 15/96 [00:37<03:23,  2.51s/it, training_loss=0.350]\u001b[A\n",
      "Epoch 2:  16%|█▌        | 15/96 [00:40<03:23,  2.51s/it, training_loss=0.334]\u001b[A\n",
      "Epoch 2:  17%|█▋        | 16/96 [00:40<03:21,  2.51s/it, training_loss=0.334]\u001b[A\n",
      "Epoch 2:  17%|█▋        | 16/96 [00:42<03:21,  2.51s/it, training_loss=0.271]\u001b[A\n",
      "Epoch 2:  18%|█▊        | 17/96 [00:42<03:18,  2.52s/it, training_loss=0.271]\u001b[A\n",
      "Epoch 2:  18%|█▊        | 17/96 [00:45<03:18,  2.52s/it, training_loss=0.314]\u001b[A\n",
      "Epoch 2:  19%|█▉        | 18/96 [00:45<03:16,  2.52s/it, training_loss=0.314]\u001b[A\n",
      "Epoch 2:  19%|█▉        | 18/96 [00:47<03:16,  2.52s/it, training_loss=0.295]\u001b[A\n",
      "Epoch 2:  20%|█▉        | 19/96 [00:47<03:13,  2.52s/it, training_loss=0.295]\u001b[A\n",
      "Epoch 2:  20%|█▉        | 19/96 [00:50<03:13,  2.52s/it, training_loss=0.273]\u001b[A\n",
      "Epoch 2:  21%|██        | 20/96 [00:50<03:11,  2.52s/it, training_loss=0.273]\u001b[A\n",
      "Epoch 2:  21%|██        | 20/96 [00:52<03:11,  2.52s/it, training_loss=0.309]\u001b[A\n",
      "Epoch 2:  22%|██▏       | 21/96 [00:52<03:08,  2.52s/it, training_loss=0.309]\u001b[A\n",
      "Epoch 2:  22%|██▏       | 21/96 [00:55<03:08,  2.52s/it, training_loss=0.309]\u001b[A\n",
      "Epoch 2:  23%|██▎       | 22/96 [00:55<03:06,  2.53s/it, training_loss=0.309]\u001b[A\n",
      "Epoch 2:  23%|██▎       | 22/96 [00:57<03:06,  2.53s/it, training_loss=0.271]\u001b[A\n",
      "Epoch 2:  24%|██▍       | 23/96 [00:57<03:04,  2.53s/it, training_loss=0.271]\u001b[A\n",
      "Epoch 2:  24%|██▍       | 23/96 [01:00<03:04,  2.53s/it, training_loss=0.232]\u001b[A\n",
      "Epoch 2:  25%|██▌       | 24/96 [01:00<03:01,  2.52s/it, training_loss=0.232]\u001b[A\n",
      "Epoch 2:  25%|██▌       | 24/96 [01:02<03:01,  2.52s/it, training_loss=0.244]\u001b[A\n",
      "Epoch 2:  26%|██▌       | 25/96 [01:02<02:58,  2.52s/it, training_loss=0.244]\u001b[A\n",
      "Epoch 2:  26%|██▌       | 25/96 [01:05<02:58,  2.52s/it, training_loss=0.306]\u001b[A\n",
      "Epoch 2:  27%|██▋       | 26/96 [01:05<02:55,  2.51s/it, training_loss=0.306]\u001b[A\n",
      "Epoch 2:  27%|██▋       | 26/96 [01:07<02:55,  2.51s/it, training_loss=0.308]\u001b[A\n",
      "Epoch 2:  28%|██▊       | 27/96 [01:07<02:53,  2.52s/it, training_loss=0.308]\u001b[A\n",
      "Epoch 2:  28%|██▊       | 27/96 [01:10<02:53,  2.52s/it, training_loss=0.322]\u001b[A\n",
      "Epoch 2:  29%|██▉       | 28/96 [01:10<02:51,  2.52s/it, training_loss=0.322]\u001b[A\n",
      "Epoch 2:  29%|██▉       | 28/96 [01:12<02:51,  2.52s/it, training_loss=0.277]\u001b[A\n",
      "Epoch 2:  30%|███       | 29/96 [01:13<02:48,  2.52s/it, training_loss=0.277]\u001b[A\n",
      "Epoch 2:  30%|███       | 29/96 [01:15<02:48,  2.52s/it, training_loss=0.218]\u001b[A\n",
      "Epoch 2:  31%|███▏      | 30/96 [01:15<02:46,  2.52s/it, training_loss=0.218]\u001b[A\n",
      "Epoch 2:  31%|███▏      | 30/96 [01:18<02:46,  2.52s/it, training_loss=0.270]\u001b[A\n",
      "Epoch 2:  32%|███▏      | 31/96 [01:18<02:43,  2.52s/it, training_loss=0.270]\u001b[A\n",
      "Epoch 2:  32%|███▏      | 31/96 [01:20<02:43,  2.52s/it, training_loss=0.304]\u001b[A\n",
      "Epoch 2:  33%|███▎      | 32/96 [01:20<02:41,  2.52s/it, training_loss=0.304]\u001b[A\n",
      "Epoch 2:  33%|███▎      | 32/96 [01:23<02:41,  2.52s/it, training_loss=0.278]\u001b[A\n",
      "Epoch 2:  34%|███▍      | 33/96 [01:23<02:38,  2.52s/it, training_loss=0.278]\u001b[A\n",
      "Epoch 2:  34%|███▍      | 33/96 [01:25<02:38,  2.52s/it, training_loss=0.246]\u001b[A\n",
      "Epoch 2:  35%|███▌      | 34/96 [01:25<02:36,  2.52s/it, training_loss=0.246]\u001b[A\n",
      "Epoch 2:  35%|███▌      | 34/96 [01:28<02:36,  2.52s/it, training_loss=0.348]\u001b[A\n",
      "Epoch 2:  36%|███▋      | 35/96 [01:28<02:34,  2.53s/it, training_loss=0.348]\u001b[A\n",
      "Epoch 2:  36%|███▋      | 35/96 [01:30<02:34,  2.53s/it, training_loss=0.254]\u001b[A\n",
      "Epoch 2:  38%|███▊      | 36/96 [01:30<02:31,  2.52s/it, training_loss=0.254]\u001b[A\n",
      "Epoch 2:  38%|███▊      | 36/96 [01:33<02:31,  2.52s/it, training_loss=0.275]\u001b[A\n",
      "Epoch 2:  39%|███▊      | 37/96 [01:33<02:28,  2.52s/it, training_loss=0.275]\u001b[A\n",
      "Epoch 2:  39%|███▊      | 37/96 [01:35<02:28,  2.52s/it, training_loss=0.274]\u001b[A\n",
      "Epoch 2:  40%|███▉      | 38/96 [01:35<02:26,  2.52s/it, training_loss=0.274]\u001b[A\n",
      "Epoch 2:  40%|███▉      | 38/96 [01:38<02:26,  2.52s/it, training_loss=0.285]\u001b[A\n",
      "Epoch 2:  41%|████      | 39/96 [01:38<02:23,  2.52s/it, training_loss=0.285]\u001b[A\n",
      "Epoch 2:  41%|████      | 39/96 [01:40<02:23,  2.52s/it, training_loss=0.239]\u001b[A\n",
      "Epoch 2:  42%|████▏     | 40/96 [01:40<02:20,  2.52s/it, training_loss=0.239]\u001b[A\n",
      "Epoch 2:  42%|████▏     | 40/96 [01:43<02:20,  2.52s/it, training_loss=0.287]\u001b[A\n",
      "Epoch 2:  43%|████▎     | 41/96 [01:43<02:18,  2.52s/it, training_loss=0.287]\u001b[A\n",
      "Epoch 2:  43%|████▎     | 41/96 [01:45<02:18,  2.52s/it, training_loss=0.282]\u001b[A\n",
      "Epoch 2:  44%|████▍     | 42/96 [01:45<02:15,  2.51s/it, training_loss=0.282]\u001b[A\n",
      "Epoch 2:  44%|████▍     | 42/96 [01:48<02:15,  2.51s/it, training_loss=0.265]\u001b[A\n",
      "Epoch 2:  45%|████▍     | 43/96 [01:48<02:13,  2.51s/it, training_loss=0.265]\u001b[A\n",
      "Epoch 2:  45%|████▍     | 43/96 [01:50<02:13,  2.51s/it, training_loss=0.282]\u001b[A\n",
      "Epoch 2:  46%|████▌     | 44/96 [01:50<02:10,  2.51s/it, training_loss=0.282]\u001b[A\n",
      "Epoch 2:  46%|████▌     | 44/96 [01:53<02:10,  2.51s/it, training_loss=0.263]\u001b[A\n",
      "Epoch 2:  47%|████▋     | 45/96 [01:53<02:08,  2.51s/it, training_loss=0.263]\u001b[A\n",
      "Epoch 2:  47%|████▋     | 45/96 [01:55<02:08,  2.51s/it, training_loss=0.313]\u001b[A\n",
      "Epoch 2:  48%|████▊     | 46/96 [01:55<02:05,  2.52s/it, training_loss=0.313]\u001b[A\n",
      "Epoch 2:  48%|████▊     | 46/96 [01:58<02:05,  2.52s/it, training_loss=0.297]\u001b[A\n",
      "Epoch 2:  49%|████▉     | 47/96 [01:58<02:03,  2.51s/it, training_loss=0.297]\u001b[A\n",
      "Epoch 2:  49%|████▉     | 47/96 [02:00<02:03,  2.51s/it, training_loss=0.264]\u001b[A\n",
      "Epoch 2:  50%|█████     | 48/96 [02:00<02:00,  2.52s/it, training_loss=0.264]\u001b[A\n",
      "Epoch 2:  50%|█████     | 48/96 [02:03<02:00,  2.52s/it, training_loss=0.262]\u001b[A\n",
      "Epoch 2:  51%|█████     | 49/96 [02:03<01:58,  2.51s/it, training_loss=0.262]\u001b[A\n",
      "Epoch 2:  51%|█████     | 49/96 [02:05<01:58,  2.51s/it, training_loss=0.287]\u001b[A\n",
      "Epoch 2:  52%|█████▏    | 50/96 [02:05<01:55,  2.51s/it, training_loss=0.287]\u001b[A\n",
      "Epoch 2:  52%|█████▏    | 50/96 [02:08<01:55,  2.51s/it, training_loss=0.235]\u001b[A\n",
      "Epoch 2:  53%|█████▎    | 51/96 [02:08<01:53,  2.51s/it, training_loss=0.235]\u001b[A\n",
      "Epoch 2:  53%|█████▎    | 51/96 [02:10<01:53,  2.51s/it, training_loss=0.239]\u001b[A\n",
      "Epoch 2:  54%|█████▍    | 52/96 [02:10<01:50,  2.52s/it, training_loss=0.239]\u001b[A\n",
      "Epoch 2:  54%|█████▍    | 52/96 [02:13<01:50,  2.52s/it, training_loss=0.271]\u001b[A\n",
      "Epoch 2:  55%|█████▌    | 53/96 [02:13<01:48,  2.52s/it, training_loss=0.271]\u001b[A\n",
      "Epoch 2:  55%|█████▌    | 53/96 [02:15<01:48,  2.52s/it, training_loss=0.304]\u001b[A\n",
      "Epoch 2:  56%|█████▋    | 54/96 [02:15<01:45,  2.52s/it, training_loss=0.304]\u001b[A\n",
      "Epoch 2:  56%|█████▋    | 54/96 [02:18<01:45,  2.52s/it, training_loss=0.322]\u001b[A\n",
      "Epoch 2:  57%|█████▋    | 55/96 [02:18<01:43,  2.52s/it, training_loss=0.322]\u001b[A\n",
      "Epoch 2:  57%|█████▋    | 55/96 [02:20<01:43,  2.52s/it, training_loss=0.236]\u001b[A\n",
      "Epoch 2:  58%|█████▊    | 56/96 [02:21<01:40,  2.52s/it, training_loss=0.236]\u001b[A\n",
      "Epoch 2:  58%|█████▊    | 56/96 [02:23<01:40,  2.52s/it, training_loss=0.309]\u001b[A\n",
      "Epoch 2:  59%|█████▉    | 57/96 [02:23<01:38,  2.52s/it, training_loss=0.309]\u001b[A\n",
      "Epoch 2:  59%|█████▉    | 57/96 [02:26<01:38,  2.52s/it, training_loss=0.210]\u001b[A\n",
      "Epoch 2:  60%|██████    | 58/96 [02:26<01:35,  2.51s/it, training_loss=0.210]\u001b[A\n",
      "Epoch 2:  60%|██████    | 58/96 [02:28<01:35,  2.51s/it, training_loss=0.256]\u001b[A\n",
      "Epoch 2:  61%|██████▏   | 59/96 [02:28<01:32,  2.51s/it, training_loss=0.256]\u001b[A\n",
      "Epoch 2:  61%|██████▏   | 59/96 [02:31<01:32,  2.51s/it, training_loss=0.246]\u001b[A\n",
      "Epoch 2:  62%|██████▎   | 60/96 [02:31<01:30,  2.51s/it, training_loss=0.246]\u001b[A\n",
      "Epoch 2:  62%|██████▎   | 60/96 [02:33<01:30,  2.51s/it, training_loss=0.251]\u001b[A\n",
      "Epoch 2:  64%|██████▎   | 61/96 [02:33<01:27,  2.51s/it, training_loss=0.251]\u001b[A\n",
      "Epoch 2:  64%|██████▎   | 61/96 [02:36<01:27,  2.51s/it, training_loss=0.320]\u001b[A\n",
      "Epoch 2:  65%|██████▍   | 62/96 [02:36<01:25,  2.51s/it, training_loss=0.320]\u001b[A\n",
      "Epoch 2:  65%|██████▍   | 62/96 [02:38<01:25,  2.51s/it, training_loss=0.225]\u001b[A\n",
      "Epoch 2:  66%|██████▌   | 63/96 [02:38<01:22,  2.51s/it, training_loss=0.225]\u001b[A\n",
      "Epoch 2:  66%|██████▌   | 63/96 [02:41<01:22,  2.51s/it, training_loss=0.239]\u001b[A\n",
      "Epoch 2:  67%|██████▋   | 64/96 [02:41<01:20,  2.52s/it, training_loss=0.239]\u001b[A\n",
      "Epoch 2:  67%|██████▋   | 64/96 [02:43<01:20,  2.52s/it, training_loss=0.332]\u001b[A\n",
      "Epoch 2:  68%|██████▊   | 65/96 [02:43<01:17,  2.51s/it, training_loss=0.332]\u001b[A\n",
      "Epoch 2:  68%|██████▊   | 65/96 [02:46<01:17,  2.51s/it, training_loss=0.276]\u001b[A\n",
      "Epoch 2:  69%|██████▉   | 66/96 [02:46<01:15,  2.52s/it, training_loss=0.276]\u001b[A\n",
      "Epoch 2:  69%|██████▉   | 66/96 [02:48<01:15,  2.52s/it, training_loss=0.232]\u001b[A\n",
      "Epoch 2:  70%|██████▉   | 67/96 [02:48<01:12,  2.51s/it, training_loss=0.232]\u001b[A\n",
      "Epoch 2:  70%|██████▉   | 67/96 [02:51<01:12,  2.51s/it, training_loss=0.176]\u001b[A\n",
      "Epoch 2:  71%|███████   | 68/96 [02:51<01:10,  2.51s/it, training_loss=0.176]\u001b[A\n",
      "Epoch 2:  71%|███████   | 68/96 [02:53<01:10,  2.51s/it, training_loss=0.220]\u001b[A\n",
      "Epoch 2:  72%|███████▏  | 69/96 [02:53<01:08,  2.52s/it, training_loss=0.220]\u001b[A\n",
      "Epoch 2:  72%|███████▏  | 69/96 [02:56<01:08,  2.52s/it, training_loss=0.253]\u001b[A\n",
      "Epoch 2:  73%|███████▎  | 70/96 [02:56<01:05,  2.52s/it, training_loss=0.253]\u001b[A\n",
      "Epoch 2:  73%|███████▎  | 70/96 [02:58<01:05,  2.52s/it, training_loss=0.284]\u001b[A\n",
      "Epoch 2:  74%|███████▍  | 71/96 [02:58<01:02,  2.52s/it, training_loss=0.284]\u001b[A\n",
      "Epoch 2:  74%|███████▍  | 71/96 [03:01<01:02,  2.52s/it, training_loss=0.281]\u001b[A\n",
      "Epoch 2:  75%|███████▌  | 72/96 [03:01<01:00,  2.52s/it, training_loss=0.281]\u001b[A\n",
      "Epoch 2:  75%|███████▌  | 72/96 [03:03<01:00,  2.52s/it, training_loss=0.188]\u001b[A\n",
      "Epoch 2:  76%|███████▌  | 73/96 [03:03<00:57,  2.52s/it, training_loss=0.188]\u001b[A\n",
      "Epoch 2:  76%|███████▌  | 73/96 [03:06<00:57,  2.52s/it, training_loss=0.254]\u001b[A\n",
      "Epoch 2:  77%|███████▋  | 74/96 [03:06<00:55,  2.52s/it, training_loss=0.254]\u001b[A\n",
      "Epoch 2:  77%|███████▋  | 74/96 [03:08<00:55,  2.52s/it, training_loss=0.289]\u001b[A\n",
      "Epoch 2:  78%|███████▊  | 75/96 [03:08<00:52,  2.52s/it, training_loss=0.289]\u001b[A\n",
      "Epoch 2:  78%|███████▊  | 75/96 [03:11<00:52,  2.52s/it, training_loss=0.235]\u001b[A\n",
      "Epoch 2:  79%|███████▉  | 76/96 [03:11<00:50,  2.52s/it, training_loss=0.235]\u001b[A\n",
      "Epoch 2:  79%|███████▉  | 76/96 [03:13<00:50,  2.52s/it, training_loss=0.243]\u001b[A\n",
      "Epoch 2:  80%|████████  | 77/96 [03:13<00:47,  2.52s/it, training_loss=0.243]\u001b[A\n",
      "Epoch 2:  80%|████████  | 77/96 [03:16<00:47,  2.52s/it, training_loss=0.292]\u001b[A\n",
      "Epoch 2:  81%|████████▏ | 78/96 [03:16<00:45,  2.51s/it, training_loss=0.292]\u001b[A\n",
      "Epoch 2:  81%|████████▏ | 78/96 [03:18<00:45,  2.51s/it, training_loss=0.322]\u001b[A\n",
      "Epoch 2:  82%|████████▏ | 79/96 [03:18<00:42,  2.52s/it, training_loss=0.322]\u001b[A\n",
      "Epoch 2:  82%|████████▏ | 79/96 [03:21<00:42,  2.52s/it, training_loss=0.227]\u001b[A\n",
      "Epoch 2:  83%|████████▎ | 80/96 [03:21<00:40,  2.51s/it, training_loss=0.227]\u001b[A\n",
      "Epoch 2:  83%|████████▎ | 80/96 [03:23<00:40,  2.51s/it, training_loss=0.229]\u001b[A\n",
      "Epoch 2:  84%|████████▍ | 81/96 [03:23<00:37,  2.51s/it, training_loss=0.229]\u001b[A\n",
      "Epoch 2:  84%|████████▍ | 81/96 [03:26<00:37,  2.51s/it, training_loss=0.282]\u001b[A\n",
      "Epoch 2:  85%|████████▌ | 82/96 [03:26<00:35,  2.51s/it, training_loss=0.282]\u001b[A\n",
      "Epoch 2:  85%|████████▌ | 82/96 [03:28<00:35,  2.51s/it, training_loss=0.280]\u001b[A\n",
      "Epoch 2:  86%|████████▋ | 83/96 [03:28<00:32,  2.51s/it, training_loss=0.280]\u001b[A\n",
      "Epoch 2:  86%|████████▋ | 83/96 [03:31<00:32,  2.51s/it, training_loss=0.269]\u001b[A\n",
      "Epoch 2:  88%|████████▊ | 84/96 [03:31<00:30,  2.52s/it, training_loss=0.269]\u001b[A\n",
      "Epoch 2:  88%|████████▊ | 84/96 [03:33<00:30,  2.52s/it, training_loss=0.181]\u001b[A\n",
      "Epoch 2:  89%|████████▊ | 85/96 [03:33<00:27,  2.52s/it, training_loss=0.181]\u001b[A\n",
      "Epoch 2:  89%|████████▊ | 85/96 [03:36<00:27,  2.52s/it, training_loss=0.305]\u001b[A\n",
      "Epoch 2:  90%|████████▉ | 86/96 [03:36<00:25,  2.51s/it, training_loss=0.305]\u001b[A\n",
      "Epoch 2:  90%|████████▉ | 86/96 [03:38<00:25,  2.51s/it, training_loss=0.251]\u001b[A\n",
      "Epoch 2:  91%|█████████ | 87/96 [03:38<00:22,  2.51s/it, training_loss=0.251]\u001b[A\n",
      "Epoch 2:  91%|█████████ | 87/96 [03:41<00:22,  2.51s/it, training_loss=0.245]\u001b[A\n",
      "Epoch 2:  92%|█████████▏| 88/96 [03:41<00:20,  2.52s/it, training_loss=0.245]\u001b[A\n",
      "Epoch 2:  92%|█████████▏| 88/96 [03:44<00:20,  2.52s/it, training_loss=0.278]\u001b[A\n",
      "Epoch 2:  93%|█████████▎| 89/96 [03:44<00:17,  2.52s/it, training_loss=0.278]\u001b[A\n",
      "Epoch 2:  93%|█████████▎| 89/96 [03:46<00:17,  2.52s/it, training_loss=0.303]\u001b[A\n",
      "Epoch 2:  94%|█████████▍| 90/96 [03:46<00:15,  2.51s/it, training_loss=0.303]\u001b[A\n",
      "Epoch 2:  94%|█████████▍| 90/96 [03:49<00:15,  2.51s/it, training_loss=0.207]\u001b[A\n",
      "Epoch 2:  95%|█████████▍| 91/96 [03:49<00:12,  2.52s/it, training_loss=0.207]\u001b[A\n",
      "Epoch 2:  95%|█████████▍| 91/96 [03:51<00:12,  2.52s/it, training_loss=0.237]\u001b[A\n",
      "Epoch 2:  96%|█████████▌| 92/96 [03:51<00:10,  2.52s/it, training_loss=0.237]\u001b[A\n",
      "Epoch 2:  96%|█████████▌| 92/96 [03:54<00:10,  2.52s/it, training_loss=0.169]\u001b[A\n",
      "Epoch 2:  97%|█████████▋| 93/96 [03:54<00:07,  2.52s/it, training_loss=0.169]\u001b[A\n",
      "Epoch 2:  97%|█████████▋| 93/96 [03:56<00:07,  2.52s/it, training_loss=0.145]\u001b[A\n",
      "Epoch 2:  98%|█████████▊| 94/96 [03:56<00:05,  2.52s/it, training_loss=0.145]\u001b[A\n",
      "Epoch 2:  98%|█████████▊| 94/96 [03:59<00:05,  2.52s/it, training_loss=0.230]\u001b[A\n",
      "Epoch 2:  99%|█████████▉| 95/96 [03:59<00:02,  2.52s/it, training_loss=0.230]\u001b[A\n",
      "Epoch 2:  99%|█████████▉| 95/96 [04:00<00:02,  2.52s/it, training_loss=0.248]\u001b[A\n",
      "Epoch 2: 100%|██████████| 96/96 [04:00<00:00,  2.25s/it, training_loss=0.248]\u001b[A\n",
      " 20%|██        | 1/5 [08:18<17:04, 256.13s/it]\n",
      "  0%|          | 0/17 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2\n",
      "Training loss: 0.8287055486192306\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  6%|▌         | 1/17 [00:00<00:14,  1.10it/s]\u001b[A\n",
      " 12%|█▏        | 2/17 [00:01<00:13,  1.10it/s]\u001b[A\n",
      " 18%|█▊        | 3/17 [00:02<00:12,  1.10it/s]\u001b[A\n",
      " 24%|██▎       | 4/17 [00:03<00:11,  1.09it/s]\u001b[A\n",
      " 29%|██▉       | 5/17 [00:04<00:10,  1.09it/s]\u001b[A\n",
      " 35%|███▌      | 6/17 [00:05<00:10,  1.09it/s]\u001b[A\n",
      " 41%|████      | 7/17 [00:06<00:09,  1.08it/s]\u001b[A\n",
      " 47%|████▋     | 8/17 [00:07<00:08,  1.08it/s]\u001b[A\n",
      " 53%|█████▎    | 9/17 [00:08<00:07,  1.08it/s]\u001b[A\n",
      " 59%|█████▉    | 10/17 [00:09<00:06,  1.09it/s]\u001b[A\n",
      " 65%|██████▍   | 11/17 [00:10<00:05,  1.08it/s]\u001b[A\n",
      " 71%|███████   | 12/17 [00:11<00:04,  1.08it/s]\u001b[A\n",
      " 76%|███████▋  | 13/17 [00:11<00:03,  1.09it/s]\u001b[A\n",
      " 82%|████████▏ | 14/17 [00:12<00:02,  1.08it/s]\u001b[A\n",
      " 88%|████████▊ | 15/17 [00:13<00:01,  1.08it/s]\u001b[A\n",
      " 94%|█████████▍| 16/17 [00:14<00:00,  1.08it/s]\u001b[A\n",
      "100%|██████████| 17/17 [00:15<00:00,  1.09it/s]\n",
      " 40%|████      | 2/5 [08:33<12:49, 256.65s/it]\n",
      "Epoch 3:   0%|          | 0/96 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.6644105078542933\n",
      "F1 Score (Weighted): 0.8309847139858879\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3:   0%|          | 0/96 [00:02<?, ?it/s, training_loss=0.208]\u001b[A\n",
      "Epoch 3:   1%|          | 1/96 [00:02<03:59,  2.52s/it, training_loss=0.208]\u001b[A\n",
      "Epoch 3:   1%|          | 1/96 [00:05<03:59,  2.52s/it, training_loss=0.174]\u001b[A\n",
      "Epoch 3:   2%|▏         | 2/96 [00:05<03:56,  2.52s/it, training_loss=0.174]\u001b[A\n",
      "Epoch 3:   2%|▏         | 2/96 [00:07<03:56,  2.52s/it, training_loss=0.298]\u001b[A\n",
      "Epoch 3:   3%|▎         | 3/96 [00:07<03:54,  2.52s/it, training_loss=0.298]\u001b[A\n",
      "Epoch 3:   3%|▎         | 3/96 [00:10<03:54,  2.52s/it, training_loss=0.248]\u001b[A\n",
      "Epoch 3:   4%|▍         | 4/96 [00:10<03:51,  2.52s/it, training_loss=0.248]\u001b[A\n",
      "Epoch 3:   4%|▍         | 4/96 [00:12<03:51,  2.52s/it, training_loss=0.156]\u001b[A\n",
      "Epoch 3:   5%|▌         | 5/96 [00:12<03:49,  2.52s/it, training_loss=0.156]\u001b[A\n",
      "Epoch 3:   5%|▌         | 5/96 [00:15<03:49,  2.52s/it, training_loss=0.172]\u001b[A\n",
      "Epoch 3:   6%|▋         | 6/96 [00:15<03:46,  2.52s/it, training_loss=0.172]\u001b[A\n",
      "Epoch 3:   6%|▋         | 6/96 [00:17<03:46,  2.52s/it, training_loss=0.209]\u001b[A\n",
      "Epoch 3:   7%|▋         | 7/96 [00:17<03:43,  2.51s/it, training_loss=0.209]\u001b[A\n",
      "Epoch 3:   7%|▋         | 7/96 [00:20<03:43,  2.51s/it, training_loss=0.162]\u001b[A\n",
      "Epoch 3:   8%|▊         | 8/96 [00:20<03:41,  2.51s/it, training_loss=0.162]\u001b[A\n",
      "Epoch 3:   8%|▊         | 8/96 [00:22<03:41,  2.51s/it, training_loss=0.207]\u001b[A\n",
      "Epoch 3:   9%|▉         | 9/96 [00:22<03:38,  2.52s/it, training_loss=0.207]\u001b[A\n",
      "Epoch 3:   9%|▉         | 9/96 [00:25<03:38,  2.52s/it, training_loss=0.163]\u001b[A\n",
      "Epoch 3:  10%|█         | 10/96 [00:25<03:36,  2.52s/it, training_loss=0.163]\u001b[A\n",
      "Epoch 3:  10%|█         | 10/96 [00:27<03:36,  2.52s/it, training_loss=0.183]\u001b[A\n",
      "Epoch 3:  11%|█▏        | 11/96 [00:27<03:33,  2.52s/it, training_loss=0.183]\u001b[A\n",
      "Epoch 3:  11%|█▏        | 11/96 [00:30<03:33,  2.52s/it, training_loss=0.315]\u001b[A\n",
      "Epoch 3:  12%|█▎        | 12/96 [00:30<03:31,  2.51s/it, training_loss=0.315]\u001b[A\n",
      "Epoch 3:  12%|█▎        | 12/96 [00:32<03:31,  2.51s/it, training_loss=0.293]\u001b[A\n",
      "Epoch 3:  14%|█▎        | 13/96 [00:32<03:28,  2.51s/it, training_loss=0.293]\u001b[A\n",
      "Epoch 3:  14%|█▎        | 13/96 [00:35<03:28,  2.51s/it, training_loss=0.169]\u001b[A\n",
      "Epoch 3:  15%|█▍        | 14/96 [00:35<03:26,  2.51s/it, training_loss=0.169]\u001b[A\n",
      "Epoch 3:  15%|█▍        | 14/96 [00:37<03:26,  2.51s/it, training_loss=0.182]\u001b[A\n",
      "Epoch 3:  16%|█▌        | 15/96 [00:37<03:23,  2.52s/it, training_loss=0.182]\u001b[A\n",
      "Epoch 3:  16%|█▌        | 15/96 [00:40<03:23,  2.52s/it, training_loss=0.235]\u001b[A\n",
      "Epoch 3:  17%|█▋        | 16/96 [00:40<03:21,  2.52s/it, training_loss=0.235]\u001b[A\n",
      "Epoch 3:  17%|█▋        | 16/96 [00:42<03:21,  2.52s/it, training_loss=0.223]\u001b[A\n",
      "Epoch 3:  18%|█▊        | 17/96 [00:42<03:18,  2.52s/it, training_loss=0.223]\u001b[A\n",
      "Epoch 3:  18%|█▊        | 17/96 [00:45<03:18,  2.52s/it, training_loss=0.198]\u001b[A\n",
      "Epoch 3:  19%|█▉        | 18/96 [00:45<03:16,  2.52s/it, training_loss=0.198]\u001b[A\n",
      "Epoch 3:  19%|█▉        | 18/96 [00:47<03:16,  2.52s/it, training_loss=0.206]\u001b[A\n",
      "Epoch 3:  20%|█▉        | 19/96 [00:47<03:14,  2.52s/it, training_loss=0.206]\u001b[A\n",
      "Epoch 3:  20%|█▉        | 19/96 [00:50<03:14,  2.52s/it, training_loss=0.142]\u001b[A\n",
      "Epoch 3:  21%|██        | 20/96 [00:50<03:11,  2.52s/it, training_loss=0.142]\u001b[A\n",
      "Epoch 3:  21%|██        | 20/96 [00:52<03:11,  2.52s/it, training_loss=0.176]\u001b[A\n",
      "Epoch 3:  22%|██▏       | 21/96 [00:52<03:09,  2.52s/it, training_loss=0.176]\u001b[A\n",
      "Epoch 3:  22%|██▏       | 21/96 [00:55<03:09,  2.52s/it, training_loss=0.154]\u001b[A\n",
      "Epoch 3:  23%|██▎       | 22/96 [00:55<03:06,  2.52s/it, training_loss=0.154]\u001b[A\n",
      "Epoch 3:  23%|██▎       | 22/96 [00:57<03:06,  2.52s/it, training_loss=0.257]\u001b[A\n",
      "Epoch 3:  24%|██▍       | 23/96 [00:57<03:04,  2.52s/it, training_loss=0.257]\u001b[A\n",
      "Epoch 3:  24%|██▍       | 23/96 [01:00<03:04,  2.52s/it, training_loss=0.164]\u001b[A\n",
      "Epoch 3:  25%|██▌       | 24/96 [01:00<03:01,  2.52s/it, training_loss=0.164]\u001b[A\n",
      "Epoch 3:  25%|██▌       | 24/96 [01:02<03:01,  2.52s/it, training_loss=0.236]\u001b[A\n",
      "Epoch 3:  26%|██▌       | 25/96 [01:02<02:58,  2.52s/it, training_loss=0.236]\u001b[A\n",
      "Epoch 3:  26%|██▌       | 25/96 [01:05<02:58,  2.52s/it, training_loss=0.197]\u001b[A\n",
      "Epoch 3:  27%|██▋       | 26/96 [01:05<02:56,  2.52s/it, training_loss=0.197]\u001b[A\n",
      "Epoch 3:  27%|██▋       | 26/96 [01:07<02:56,  2.52s/it, training_loss=0.272]\u001b[A\n",
      "Epoch 3:  28%|██▊       | 27/96 [01:07<02:53,  2.52s/it, training_loss=0.272]\u001b[A\n",
      "Epoch 3:  28%|██▊       | 27/96 [01:10<02:53,  2.52s/it, training_loss=0.173]\u001b[A\n",
      "Epoch 3:  29%|██▉       | 28/96 [01:10<02:50,  2.51s/it, training_loss=0.173]\u001b[A\n",
      "Epoch 3:  29%|██▉       | 28/96 [01:12<02:50,  2.51s/it, training_loss=0.180]\u001b[A\n",
      "Epoch 3:  30%|███       | 29/96 [01:12<02:48,  2.51s/it, training_loss=0.180]\u001b[A\n",
      "Epoch 3:  30%|███       | 29/96 [01:15<02:48,  2.51s/it, training_loss=0.156]\u001b[A\n",
      "Epoch 3:  31%|███▏      | 30/96 [01:15<02:45,  2.51s/it, training_loss=0.156]\u001b[A\n",
      "Epoch 3:  31%|███▏      | 30/96 [01:18<02:45,  2.51s/it, training_loss=0.235]\u001b[A\n",
      "Epoch 3:  32%|███▏      | 31/96 [01:18<02:43,  2.51s/it, training_loss=0.235]\u001b[A\n",
      "Epoch 3:  32%|███▏      | 31/96 [01:20<02:43,  2.51s/it, training_loss=0.244]\u001b[A\n",
      "Epoch 3:  33%|███▎      | 32/96 [01:20<02:41,  2.52s/it, training_loss=0.244]\u001b[A\n",
      "Epoch 3:  33%|███▎      | 32/96 [01:23<02:41,  2.52s/it, training_loss=0.176]\u001b[A\n",
      "Epoch 3:  34%|███▍      | 33/96 [01:23<02:38,  2.52s/it, training_loss=0.176]\u001b[A\n",
      "Epoch 3:  34%|███▍      | 33/96 [01:25<02:38,  2.52s/it, training_loss=0.270]\u001b[A\n",
      "Epoch 3:  35%|███▌      | 34/96 [01:25<02:36,  2.52s/it, training_loss=0.270]\u001b[A\n",
      "Epoch 3:  35%|███▌      | 34/96 [01:28<02:36,  2.52s/it, training_loss=0.135]\u001b[A\n",
      "Epoch 3:  36%|███▋      | 35/96 [01:28<02:33,  2.52s/it, training_loss=0.135]\u001b[A\n",
      "Epoch 3:  36%|███▋      | 35/96 [01:30<02:33,  2.52s/it, training_loss=0.176]\u001b[A\n",
      "Epoch 3:  38%|███▊      | 36/96 [01:30<02:31,  2.52s/it, training_loss=0.176]\u001b[A\n",
      "Epoch 3:  38%|███▊      | 36/96 [01:33<02:31,  2.52s/it, training_loss=0.238]\u001b[A\n",
      "Epoch 3:  39%|███▊      | 37/96 [01:33<02:28,  2.52s/it, training_loss=0.238]\u001b[A\n",
      "Epoch 3:  39%|███▊      | 37/96 [01:35<02:28,  2.52s/it, training_loss=0.199]\u001b[A\n",
      "Epoch 3:  40%|███▉      | 38/96 [01:35<02:26,  2.52s/it, training_loss=0.199]\u001b[A\n",
      "Epoch 3:  40%|███▉      | 38/96 [01:38<02:26,  2.52s/it, training_loss=0.195]\u001b[A\n",
      "Epoch 3:  41%|████      | 39/96 [01:38<02:23,  2.52s/it, training_loss=0.195]\u001b[A\n",
      "Epoch 3:  41%|████      | 39/96 [01:40<02:23,  2.52s/it, training_loss=0.182]\u001b[A\n",
      "Epoch 3:  42%|████▏     | 40/96 [01:40<02:21,  2.52s/it, training_loss=0.182]\u001b[A\n",
      "Epoch 3:  42%|████▏     | 40/96 [01:43<02:21,  2.52s/it, training_loss=0.197]\u001b[A\n",
      "Epoch 3:  43%|████▎     | 41/96 [01:43<02:18,  2.52s/it, training_loss=0.197]\u001b[A\n",
      "Epoch 3:  43%|████▎     | 41/96 [01:45<02:18,  2.52s/it, training_loss=0.174]\u001b[A\n",
      "Epoch 3:  44%|████▍     | 42/96 [01:45<02:16,  2.52s/it, training_loss=0.174]\u001b[A\n",
      "Epoch 3:  44%|████▍     | 42/96 [01:48<02:16,  2.52s/it, training_loss=0.203]\u001b[A\n",
      "Epoch 3:  45%|████▍     | 43/96 [01:48<02:13,  2.52s/it, training_loss=0.203]\u001b[A\n",
      "Epoch 3:  45%|████▍     | 43/96 [01:50<02:13,  2.52s/it, training_loss=0.191]\u001b[A\n",
      "Epoch 3:  46%|████▌     | 44/96 [01:50<02:10,  2.51s/it, training_loss=0.191]\u001b[A\n",
      "Epoch 3:  46%|████▌     | 44/96 [01:53<02:10,  2.51s/it, training_loss=0.163]\u001b[A\n",
      "Epoch 3:  47%|████▋     | 45/96 [01:53<02:08,  2.51s/it, training_loss=0.163]\u001b[A\n",
      "Epoch 3:  47%|████▋     | 45/96 [01:55<02:08,  2.51s/it, training_loss=0.154]\u001b[A\n",
      "Epoch 3:  48%|████▊     | 46/96 [01:55<02:05,  2.51s/it, training_loss=0.154]\u001b[A\n",
      "Epoch 3:  48%|████▊     | 46/96 [01:58<02:05,  2.51s/it, training_loss=0.146]\u001b[A\n",
      "Epoch 3:  49%|████▉     | 47/96 [01:58<02:02,  2.51s/it, training_loss=0.146]\u001b[A\n",
      "Epoch 3:  49%|████▉     | 47/96 [02:00<02:02,  2.51s/it, training_loss=0.196]\u001b[A\n",
      "Epoch 3:  50%|█████     | 48/96 [02:00<02:00,  2.51s/it, training_loss=0.196]\u001b[A\n",
      "Epoch 3:  50%|█████     | 48/96 [02:03<02:00,  2.51s/it, training_loss=0.202]\u001b[A\n",
      "Epoch 3:  51%|█████     | 49/96 [02:03<01:58,  2.51s/it, training_loss=0.202]\u001b[A\n",
      "Epoch 3:  51%|█████     | 49/96 [02:05<01:58,  2.51s/it, training_loss=0.141]\u001b[A\n",
      "Epoch 3:  52%|█████▏    | 50/96 [02:05<01:55,  2.51s/it, training_loss=0.141]\u001b[A\n",
      "Epoch 3:  52%|█████▏    | 50/96 [02:08<01:55,  2.51s/it, training_loss=0.201]\u001b[A\n",
      "Epoch 3:  53%|█████▎    | 51/96 [02:08<01:53,  2.51s/it, training_loss=0.201]\u001b[A\n",
      "Epoch 3:  53%|█████▎    | 51/96 [02:10<01:53,  2.51s/it, training_loss=0.174]\u001b[A\n",
      "Epoch 3:  54%|█████▍    | 52/96 [02:10<01:50,  2.52s/it, training_loss=0.174]\u001b[A\n",
      "Epoch 3:  54%|█████▍    | 52/96 [02:13<01:50,  2.52s/it, training_loss=0.218]\u001b[A\n",
      "Epoch 3:  55%|█████▌    | 53/96 [02:13<01:48,  2.51s/it, training_loss=0.218]\u001b[A\n",
      "Epoch 3:  55%|█████▌    | 53/96 [02:15<01:48,  2.51s/it, training_loss=0.209]\u001b[A\n",
      "Epoch 3:  56%|█████▋    | 54/96 [02:15<01:45,  2.51s/it, training_loss=0.209]\u001b[A\n",
      "Epoch 3:  56%|█████▋    | 54/96 [02:18<01:45,  2.51s/it, training_loss=0.150]\u001b[A\n",
      "Epoch 3:  57%|█████▋    | 55/96 [02:18<01:43,  2.52s/it, training_loss=0.150]\u001b[A\n",
      "Epoch 3:  57%|█████▋    | 55/96 [02:20<01:43,  2.52s/it, training_loss=0.202]\u001b[A\n",
      "Epoch 3:  58%|█████▊    | 56/96 [02:20<01:40,  2.52s/it, training_loss=0.202]\u001b[A\n",
      "Epoch 3:  58%|█████▊    | 56/96 [02:23<01:40,  2.52s/it, training_loss=0.227]\u001b[A\n",
      "Epoch 3:  59%|█████▉    | 57/96 [02:23<01:38,  2.52s/it, training_loss=0.227]\u001b[A\n",
      "Epoch 3:  59%|█████▉    | 57/96 [02:25<01:38,  2.52s/it, training_loss=0.212]\u001b[A\n",
      "Epoch 3:  60%|██████    | 58/96 [02:25<01:35,  2.53s/it, training_loss=0.212]\u001b[A\n",
      "Epoch 3:  60%|██████    | 58/96 [02:28<01:35,  2.53s/it, training_loss=0.207]\u001b[A\n",
      "Epoch 3:  61%|██████▏   | 59/96 [02:28<01:33,  2.52s/it, training_loss=0.207]\u001b[A\n",
      "Epoch 3:  61%|██████▏   | 59/96 [02:31<01:33,  2.52s/it, training_loss=0.184]\u001b[A\n",
      "Epoch 3:  62%|██████▎   | 60/96 [02:31<01:30,  2.52s/it, training_loss=0.184]\u001b[A\n",
      "Epoch 3:  62%|██████▎   | 60/96 [02:33<01:30,  2.52s/it, training_loss=0.170]\u001b[A\n",
      "Epoch 3:  64%|██████▎   | 61/96 [02:33<01:28,  2.52s/it, training_loss=0.170]\u001b[A\n",
      "Epoch 3:  64%|██████▎   | 61/96 [02:36<01:28,  2.52s/it, training_loss=0.157]\u001b[A\n",
      "Epoch 3:  65%|██████▍   | 62/96 [02:36<01:25,  2.52s/it, training_loss=0.157]\u001b[A\n",
      "Epoch 3:  65%|██████▍   | 62/96 [02:38<01:25,  2.52s/it, training_loss=0.232]\u001b[A\n",
      "Epoch 3:  66%|██████▌   | 63/96 [02:38<01:23,  2.52s/it, training_loss=0.232]\u001b[A\n",
      "Epoch 3:  66%|██████▌   | 63/96 [02:41<01:23,  2.52s/it, training_loss=0.231]\u001b[A\n",
      "Epoch 3:  67%|██████▋   | 64/96 [02:41<01:20,  2.52s/it, training_loss=0.231]\u001b[A\n",
      "Epoch 3:  67%|██████▋   | 64/96 [02:43<01:20,  2.52s/it, training_loss=0.161]\u001b[A\n",
      "Epoch 3:  68%|██████▊   | 65/96 [02:43<01:17,  2.52s/it, training_loss=0.161]\u001b[A\n",
      "Epoch 3:  68%|██████▊   | 65/96 [02:46<01:17,  2.52s/it, training_loss=0.195]\u001b[A\n",
      "Epoch 3:  69%|██████▉   | 66/96 [02:46<01:15,  2.52s/it, training_loss=0.195]\u001b[A\n",
      "Epoch 3:  69%|██████▉   | 66/96 [02:48<01:15,  2.52s/it, training_loss=0.199]\u001b[A\n",
      "Epoch 3:  70%|██████▉   | 67/96 [02:48<01:12,  2.52s/it, training_loss=0.199]\u001b[A\n",
      "Epoch 3:  70%|██████▉   | 67/96 [02:51<01:12,  2.52s/it, training_loss=0.189]\u001b[A\n",
      "Epoch 3:  71%|███████   | 68/96 [02:51<01:10,  2.52s/it, training_loss=0.189]\u001b[A\n",
      "Epoch 3:  71%|███████   | 68/96 [02:53<01:10,  2.52s/it, training_loss=0.155]\u001b[A\n",
      "Epoch 3:  72%|███████▏  | 69/96 [02:53<01:08,  2.52s/it, training_loss=0.155]\u001b[A\n",
      "Epoch 3:  72%|███████▏  | 69/96 [02:56<01:08,  2.52s/it, training_loss=0.208]\u001b[A\n",
      "Epoch 3:  73%|███████▎  | 70/96 [02:56<01:05,  2.52s/it, training_loss=0.208]\u001b[A\n",
      "Epoch 3:  73%|███████▎  | 70/96 [02:58<01:05,  2.52s/it, training_loss=0.136]\u001b[A\n",
      "Epoch 3:  74%|███████▍  | 71/96 [02:58<01:03,  2.52s/it, training_loss=0.136]\u001b[A\n",
      "Epoch 3:  74%|███████▍  | 71/96 [03:01<01:03,  2.52s/it, training_loss=0.158]\u001b[A\n",
      "Epoch 3:  75%|███████▌  | 72/96 [03:01<01:00,  2.52s/it, training_loss=0.158]\u001b[A\n",
      "Epoch 3:  75%|███████▌  | 72/96 [03:03<01:00,  2.52s/it, training_loss=0.236]\u001b[A\n",
      "Epoch 3:  76%|███████▌  | 73/96 [03:03<00:58,  2.52s/it, training_loss=0.236]\u001b[A\n",
      "Epoch 3:  76%|███████▌  | 73/96 [03:06<00:58,  2.52s/it, training_loss=0.155]\u001b[A\n",
      "Epoch 3:  77%|███████▋  | 74/96 [03:06<00:55,  2.52s/it, training_loss=0.155]\u001b[A\n",
      "Epoch 3:  77%|███████▋  | 74/96 [03:08<00:55,  2.52s/it, training_loss=0.186]\u001b[A\n",
      "Epoch 3:  78%|███████▊  | 75/96 [03:08<00:53,  2.52s/it, training_loss=0.186]\u001b[A\n",
      "Epoch 3:  78%|███████▊  | 75/96 [03:11<00:53,  2.52s/it, training_loss=0.168]\u001b[A\n",
      "Epoch 3:  79%|███████▉  | 76/96 [03:11<00:50,  2.52s/it, training_loss=0.168]\u001b[A\n",
      "Epoch 3:  79%|███████▉  | 76/96 [03:13<00:50,  2.52s/it, training_loss=0.109]\u001b[A\n",
      "Epoch 3:  80%|████████  | 77/96 [03:13<00:47,  2.52s/it, training_loss=0.109]\u001b[A\n",
      "Epoch 3:  80%|████████  | 77/96 [03:16<00:47,  2.52s/it, training_loss=0.156]\u001b[A\n",
      "Epoch 3:  81%|████████▏ | 78/96 [03:16<00:45,  2.52s/it, training_loss=0.156]\u001b[A\n",
      "Epoch 3:  81%|████████▏ | 78/96 [03:18<00:45,  2.52s/it, training_loss=0.193]\u001b[A\n",
      "Epoch 3:  82%|████████▏ | 79/96 [03:18<00:42,  2.52s/it, training_loss=0.193]\u001b[A\n",
      "Epoch 3:  82%|████████▏ | 79/96 [03:21<00:42,  2.52s/it, training_loss=0.156]\u001b[A\n",
      "Epoch 3:  83%|████████▎ | 80/96 [03:21<00:40,  2.52s/it, training_loss=0.156]\u001b[A\n",
      "Epoch 3:  83%|████████▎ | 80/96 [03:23<00:40,  2.52s/it, training_loss=0.162]\u001b[A\n",
      "Epoch 3:  84%|████████▍ | 81/96 [03:23<00:37,  2.52s/it, training_loss=0.162]\u001b[A\n",
      "Epoch 3:  84%|████████▍ | 81/96 [03:26<00:37,  2.52s/it, training_loss=0.158]\u001b[A\n",
      "Epoch 3:  85%|████████▌ | 82/96 [03:26<00:35,  2.52s/it, training_loss=0.158]\u001b[A\n",
      "Epoch 3:  85%|████████▌ | 82/96 [03:28<00:35,  2.52s/it, training_loss=0.161]\u001b[A\n",
      "Epoch 3:  86%|████████▋ | 83/96 [03:28<00:32,  2.52s/it, training_loss=0.161]\u001b[A\n",
      "Epoch 3:  86%|████████▋ | 83/96 [03:31<00:32,  2.52s/it, training_loss=0.217]\u001b[A\n",
      "Epoch 3:  88%|████████▊ | 84/96 [03:31<00:30,  2.52s/it, training_loss=0.217]\u001b[A\n",
      "Epoch 3:  88%|████████▊ | 84/96 [03:34<00:30,  2.52s/it, training_loss=0.172]\u001b[A\n",
      "Epoch 3:  89%|████████▊ | 85/96 [03:34<00:27,  2.52s/it, training_loss=0.172]\u001b[A\n",
      "Epoch 3:  89%|████████▊ | 85/96 [03:36<00:27,  2.52s/it, training_loss=0.140]\u001b[A\n",
      "Epoch 3:  90%|████████▉ | 86/96 [03:36<00:25,  2.52s/it, training_loss=0.140]\u001b[A\n",
      "Epoch 3:  90%|████████▉ | 86/96 [03:39<00:25,  2.52s/it, training_loss=0.147]\u001b[A\n",
      "Epoch 3:  91%|█████████ | 87/96 [03:39<00:22,  2.52s/it, training_loss=0.147]\u001b[A\n",
      "Epoch 3:  91%|█████████ | 87/96 [03:41<00:22,  2.52s/it, training_loss=0.180]\u001b[A\n",
      "Epoch 3:  92%|█████████▏| 88/96 [03:41<00:20,  2.53s/it, training_loss=0.180]\u001b[A\n",
      "Epoch 3:  92%|█████████▏| 88/96 [03:44<00:20,  2.53s/it, training_loss=0.128]\u001b[A\n",
      "Epoch 3:  93%|█████████▎| 89/96 [03:44<00:17,  2.52s/it, training_loss=0.128]\u001b[A\n",
      "Epoch 3:  93%|█████████▎| 89/96 [03:46<00:17,  2.52s/it, training_loss=0.094]\u001b[A\n",
      "Epoch 3:  94%|█████████▍| 90/96 [03:46<00:15,  2.52s/it, training_loss=0.094]\u001b[A\n",
      "Epoch 3:  94%|█████████▍| 90/96 [03:49<00:15,  2.52s/it, training_loss=0.166]\u001b[A\n",
      "Epoch 3:  95%|█████████▍| 91/96 [03:49<00:12,  2.52s/it, training_loss=0.166]\u001b[A\n",
      "Epoch 3:  95%|█████████▍| 91/96 [03:51<00:12,  2.52s/it, training_loss=0.162]\u001b[A\n",
      "Epoch 3:  96%|█████████▌| 92/96 [03:51<00:10,  2.53s/it, training_loss=0.162]\u001b[A\n",
      "Epoch 3:  96%|█████████▌| 92/96 [03:54<00:10,  2.53s/it, training_loss=0.168]\u001b[A\n",
      "Epoch 3:  97%|█████████▋| 93/96 [03:54<00:07,  2.52s/it, training_loss=0.168]\u001b[A\n",
      "Epoch 3:  97%|█████████▋| 93/96 [03:56<00:07,  2.52s/it, training_loss=0.106]\u001b[A\n",
      "Epoch 3:  98%|█████████▊| 94/96 [03:56<00:05,  2.52s/it, training_loss=0.106]\u001b[A\n",
      "Epoch 3:  98%|█████████▊| 94/96 [03:59<00:05,  2.52s/it, training_loss=0.130]\u001b[A\n",
      "Epoch 3:  99%|█████████▉| 95/96 [03:59<00:02,  2.52s/it, training_loss=0.130]\u001b[A\n",
      "Epoch 3:  99%|█████████▉| 95/96 [04:00<00:02,  2.52s/it, training_loss=0.118]\u001b[A\n",
      "Epoch 3: 100%|██████████| 96/96 [04:00<00:00,  2.25s/it, training_loss=0.118]\u001b[A\n",
      " 40%|████      | 2/5 [12:36<12:49, 256.65s/it]\n",
      "  0%|          | 0/17 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3\n",
      "Training loss: 0.5584449792901675\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  6%|▌         | 1/17 [00:00<00:14,  1.10it/s]\u001b[A\n",
      " 12%|█▏        | 2/17 [00:01<00:13,  1.10it/s]\u001b[A\n",
      " 18%|█▊        | 3/17 [00:02<00:12,  1.09it/s]\u001b[A\n",
      " 24%|██▎       | 4/17 [00:03<00:11,  1.09it/s]\u001b[A\n",
      " 29%|██▉       | 5/17 [00:04<00:11,  1.09it/s]\u001b[A\n",
      " 35%|███▌      | 6/17 [00:05<00:10,  1.09it/s]\u001b[A\n",
      " 41%|████      | 7/17 [00:06<00:09,  1.09it/s]\u001b[A\n",
      " 47%|████▋     | 8/17 [00:07<00:08,  1.09it/s]\u001b[A\n",
      " 53%|█████▎    | 9/17 [00:08<00:07,  1.09it/s]\u001b[A\n",
      " 59%|█████▉    | 10/17 [00:09<00:06,  1.09it/s]\u001b[A\n",
      " 65%|██████▍   | 11/17 [00:10<00:05,  1.09it/s]\u001b[A\n",
      " 71%|███████   | 12/17 [00:11<00:04,  1.09it/s]\u001b[A\n",
      " 76%|███████▋  | 13/17 [00:11<00:03,  1.09it/s]\u001b[A\n",
      " 82%|████████▏ | 14/17 [00:12<00:02,  1.09it/s]\u001b[A\n",
      " 88%|████████▊ | 15/17 [00:13<00:01,  1.08it/s]\u001b[A\n",
      " 94%|█████████▍| 16/17 [00:14<00:00,  1.08it/s]\u001b[A\n",
      "100%|██████████| 17/17 [00:15<00:00,  1.09it/s]\n",
      " 60%|██████    | 3/5 [12:51<08:34, 257.05s/it]\n",
      "Epoch 4:   0%|          | 0/96 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.5136461108922958\n",
      "F1 Score (Weighted): 0.873722961670262\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4:   0%|          | 0/96 [00:02<?, ?it/s, training_loss=0.118]\u001b[A\n",
      "Epoch 4:   1%|          | 1/96 [00:02<04:00,  2.53s/it, training_loss=0.118]\u001b[A\n",
      "Epoch 4:   1%|          | 1/96 [00:05<04:00,  2.53s/it, training_loss=0.117]\u001b[A\n",
      "Epoch 4:   2%|▏         | 2/96 [00:05<03:57,  2.53s/it, training_loss=0.117]\u001b[A\n",
      "Epoch 4:   2%|▏         | 2/96 [00:07<03:57,  2.53s/it, training_loss=0.178]\u001b[A\n",
      "Epoch 4:   3%|▎         | 3/96 [00:07<03:54,  2.52s/it, training_loss=0.178]\u001b[A\n",
      "Epoch 4:   3%|▎         | 3/96 [00:10<03:54,  2.52s/it, training_loss=0.124]\u001b[A\n",
      "Epoch 4:   4%|▍         | 4/96 [00:10<03:52,  2.53s/it, training_loss=0.124]\u001b[A\n",
      "Epoch 4:   4%|▍         | 4/96 [00:12<03:52,  2.53s/it, training_loss=0.136]\u001b[A\n",
      "Epoch 4:   5%|▌         | 5/96 [00:12<03:49,  2.53s/it, training_loss=0.136]\u001b[A\n",
      "Epoch 4:   5%|▌         | 5/96 [00:15<03:49,  2.53s/it, training_loss=0.135]\u001b[A\n",
      "Epoch 4:   6%|▋         | 6/96 [00:15<03:47,  2.53s/it, training_loss=0.135]\u001b[A\n",
      "Epoch 4:   6%|▋         | 6/96 [00:17<03:47,  2.53s/it, training_loss=0.122]\u001b[A\n",
      "Epoch 4:   7%|▋         | 7/96 [00:17<03:44,  2.52s/it, training_loss=0.122]\u001b[A\n",
      "Epoch 4:   7%|▋         | 7/96 [00:20<03:44,  2.52s/it, training_loss=0.115]\u001b[A\n",
      "Epoch 4:   8%|▊         | 8/96 [00:20<03:42,  2.53s/it, training_loss=0.115]\u001b[A\n",
      "Epoch 4:   8%|▊         | 8/96 [00:22<03:42,  2.53s/it, training_loss=0.112]\u001b[A\n",
      "Epoch 4:   9%|▉         | 9/96 [00:22<03:39,  2.53s/it, training_loss=0.112]\u001b[A\n",
      "Epoch 4:   9%|▉         | 9/96 [00:25<03:39,  2.53s/it, training_loss=0.163]\u001b[A\n",
      "Epoch 4:  10%|█         | 10/96 [00:25<03:37,  2.52s/it, training_loss=0.163]\u001b[A\n",
      "Epoch 4:  10%|█         | 10/96 [00:27<03:37,  2.52s/it, training_loss=0.106]\u001b[A\n",
      "Epoch 4:  11%|█▏        | 11/96 [00:27<03:34,  2.52s/it, training_loss=0.106]\u001b[A\n",
      "Epoch 4:  11%|█▏        | 11/96 [00:30<03:34,  2.52s/it, training_loss=0.106]\u001b[A\n",
      "Epoch 4:  12%|█▎        | 12/96 [00:30<03:31,  2.52s/it, training_loss=0.106]\u001b[A\n",
      "Epoch 4:  12%|█▎        | 12/96 [00:32<03:31,  2.52s/it, training_loss=0.158]\u001b[A\n",
      "Epoch 4:  14%|█▎        | 13/96 [00:32<03:29,  2.52s/it, training_loss=0.158]\u001b[A\n",
      "Epoch 4:  14%|█▎        | 13/96 [00:35<03:29,  2.52s/it, training_loss=0.176]\u001b[A\n",
      "Epoch 4:  15%|█▍        | 14/96 [00:35<03:26,  2.52s/it, training_loss=0.176]\u001b[A\n",
      "Epoch 4:  15%|█▍        | 14/96 [00:37<03:26,  2.52s/it, training_loss=0.110]\u001b[A\n",
      "Epoch 4:  16%|█▌        | 15/96 [00:37<03:24,  2.52s/it, training_loss=0.110]\u001b[A\n",
      "Epoch 4:  16%|█▌        | 15/96 [00:40<03:24,  2.52s/it, training_loss=0.116]\u001b[A\n",
      "Epoch 4:  17%|█▋        | 16/96 [00:40<03:21,  2.52s/it, training_loss=0.116]\u001b[A\n",
      "Epoch 4:  17%|█▋        | 16/96 [00:42<03:21,  2.52s/it, training_loss=0.125]\u001b[A\n",
      "Epoch 4:  18%|█▊        | 17/96 [00:42<03:19,  2.52s/it, training_loss=0.125]\u001b[A\n",
      "Epoch 4:  18%|█▊        | 17/96 [00:45<03:19,  2.52s/it, training_loss=0.112]\u001b[A\n",
      "Epoch 4:  19%|█▉        | 18/96 [00:45<03:16,  2.53s/it, training_loss=0.112]\u001b[A\n",
      "Epoch 4:  19%|█▉        | 18/96 [00:47<03:16,  2.53s/it, training_loss=0.085]\u001b[A\n",
      "Epoch 4:  20%|█▉        | 19/96 [00:47<03:14,  2.52s/it, training_loss=0.085]\u001b[A\n",
      "Epoch 4:  20%|█▉        | 19/96 [00:50<03:14,  2.52s/it, training_loss=0.209]\u001b[A\n",
      "Epoch 4:  21%|██        | 20/96 [00:50<03:11,  2.52s/it, training_loss=0.209]\u001b[A\n",
      "Epoch 4:  21%|██        | 20/96 [00:52<03:11,  2.52s/it, training_loss=0.186]\u001b[A\n",
      "Epoch 4:  22%|██▏       | 21/96 [00:52<03:09,  2.52s/it, training_loss=0.186]\u001b[A\n",
      "Epoch 4:  22%|██▏       | 21/96 [00:55<03:09,  2.52s/it, training_loss=0.162]\u001b[A\n",
      "Epoch 4:  23%|██▎       | 22/96 [00:55<03:07,  2.53s/it, training_loss=0.162]\u001b[A\n",
      "Epoch 4:  23%|██▎       | 22/96 [00:58<03:07,  2.53s/it, training_loss=0.113]\u001b[A\n",
      "Epoch 4:  24%|██▍       | 23/96 [00:58<03:04,  2.52s/it, training_loss=0.113]\u001b[A\n",
      "Epoch 4:  24%|██▍       | 23/96 [01:00<03:04,  2.52s/it, training_loss=0.104]\u001b[A\n",
      "Epoch 4:  25%|██▌       | 24/96 [01:00<03:01,  2.52s/it, training_loss=0.104]\u001b[A\n",
      "Epoch 4:  25%|██▌       | 24/96 [01:03<03:01,  2.52s/it, training_loss=0.145]\u001b[A\n",
      "Epoch 4:  26%|██▌       | 25/96 [01:03<02:58,  2.52s/it, training_loss=0.145]\u001b[A\n",
      "Epoch 4:  26%|██▌       | 25/96 [01:05<02:58,  2.52s/it, training_loss=0.124]\u001b[A\n",
      "Epoch 4:  27%|██▋       | 26/96 [01:05<02:56,  2.52s/it, training_loss=0.124]\u001b[A\n",
      "Epoch 4:  27%|██▋       | 26/96 [01:08<02:56,  2.52s/it, training_loss=0.148]\u001b[A\n",
      "Epoch 4:  28%|██▊       | 27/96 [01:08<02:53,  2.51s/it, training_loss=0.148]\u001b[A\n",
      "Epoch 4:  28%|██▊       | 27/96 [01:10<02:53,  2.51s/it, training_loss=0.196]\u001b[A\n",
      "Epoch 4:  29%|██▉       | 28/96 [01:10<02:50,  2.51s/it, training_loss=0.196]\u001b[A\n",
      "Epoch 4:  29%|██▉       | 28/96 [01:13<02:50,  2.51s/it, training_loss=0.120]\u001b[A\n",
      "Epoch 4:  30%|███       | 29/96 [01:13<02:48,  2.51s/it, training_loss=0.120]\u001b[A\n",
      "Epoch 4:  30%|███       | 29/96 [01:15<02:48,  2.51s/it, training_loss=0.135]\u001b[A\n",
      "Epoch 4:  31%|███▏      | 30/96 [01:15<02:46,  2.52s/it, training_loss=0.135]\u001b[A\n",
      "Epoch 4:  31%|███▏      | 30/96 [01:18<02:46,  2.52s/it, training_loss=0.110]\u001b[A\n",
      "Epoch 4:  32%|███▏      | 31/96 [01:18<02:43,  2.52s/it, training_loss=0.110]\u001b[A\n",
      "Epoch 4:  32%|███▏      | 31/96 [01:20<02:43,  2.52s/it, training_loss=0.109]\u001b[A\n",
      "Epoch 4:  33%|███▎      | 32/96 [01:20<02:41,  2.52s/it, training_loss=0.109]\u001b[A\n",
      "Epoch 4:  33%|███▎      | 32/96 [01:23<02:41,  2.52s/it, training_loss=0.129]\u001b[A\n",
      "Epoch 4:  34%|███▍      | 33/96 [01:23<02:38,  2.52s/it, training_loss=0.129]\u001b[A\n",
      "Epoch 4:  34%|███▍      | 33/96 [01:25<02:38,  2.52s/it, training_loss=0.084]\u001b[A\n",
      "Epoch 4:  35%|███▌      | 34/96 [01:25<02:36,  2.52s/it, training_loss=0.084]\u001b[A\n",
      "Epoch 4:  35%|███▌      | 34/96 [01:28<02:36,  2.52s/it, training_loss=0.141]\u001b[A\n",
      "Epoch 4:  36%|███▋      | 35/96 [01:28<02:33,  2.52s/it, training_loss=0.141]\u001b[A\n",
      "Epoch 4:  36%|███▋      | 35/96 [01:30<02:33,  2.52s/it, training_loss=0.139]\u001b[A\n",
      "Epoch 4:  38%|███▊      | 36/96 [01:30<02:31,  2.53s/it, training_loss=0.139]\u001b[A\n",
      "Epoch 4:  38%|███▊      | 36/96 [01:33<02:31,  2.53s/it, training_loss=0.124]\u001b[A\n",
      "Epoch 4:  39%|███▊      | 37/96 [01:33<02:29,  2.53s/it, training_loss=0.124]\u001b[A\n",
      "Epoch 4:  39%|███▊      | 37/96 [01:35<02:29,  2.53s/it, training_loss=0.140]\u001b[A\n",
      "Epoch 4:  40%|███▉      | 38/96 [01:35<02:26,  2.53s/it, training_loss=0.140]\u001b[A\n",
      "Epoch 4:  40%|███▉      | 38/96 [01:38<02:26,  2.53s/it, training_loss=0.141]\u001b[A\n",
      "Epoch 4:  41%|████      | 39/96 [01:38<02:24,  2.53s/it, training_loss=0.141]\u001b[A\n",
      "Epoch 4:  41%|████      | 39/96 [01:40<02:24,  2.53s/it, training_loss=0.160]\u001b[A\n",
      "Epoch 4:  42%|████▏     | 40/96 [01:40<02:21,  2.53s/it, training_loss=0.160]\u001b[A\n",
      "Epoch 4:  42%|████▏     | 40/96 [01:43<02:21,  2.53s/it, training_loss=0.218]\u001b[A\n",
      "Epoch 4:  43%|████▎     | 41/96 [01:43<02:18,  2.53s/it, training_loss=0.218]\u001b[A\n",
      "Epoch 4:  43%|████▎     | 41/96 [01:45<02:18,  2.53s/it, training_loss=0.102]\u001b[A\n",
      "Epoch 4:  44%|████▍     | 42/96 [01:45<02:16,  2.52s/it, training_loss=0.102]\u001b[A\n",
      "Epoch 4:  44%|████▍     | 42/96 [01:48<02:16,  2.52s/it, training_loss=0.169]\u001b[A\n",
      "Epoch 4:  45%|████▍     | 43/96 [01:48<02:13,  2.52s/it, training_loss=0.169]\u001b[A\n",
      "Epoch 4:  45%|████▍     | 43/96 [01:51<02:13,  2.52s/it, training_loss=0.108]\u001b[A\n",
      "Epoch 4:  46%|████▌     | 44/96 [01:51<02:11,  2.53s/it, training_loss=0.108]\u001b[A\n",
      "Epoch 4:  46%|████▌     | 44/96 [01:53<02:11,  2.53s/it, training_loss=0.147]\u001b[A\n",
      "Epoch 4:  47%|████▋     | 45/96 [01:53<02:09,  2.53s/it, training_loss=0.147]\u001b[A\n",
      "Epoch 4:  47%|████▋     | 45/96 [01:56<02:09,  2.53s/it, training_loss=0.124]\u001b[A\n",
      "Epoch 4:  48%|████▊     | 46/96 [01:56<02:06,  2.53s/it, training_loss=0.124]\u001b[A\n",
      "Epoch 4:  48%|████▊     | 46/96 [01:58<02:06,  2.53s/it, training_loss=0.205]\u001b[A\n",
      "Epoch 4:  49%|████▉     | 47/96 [01:58<02:03,  2.52s/it, training_loss=0.205]\u001b[A\n",
      "Epoch 4:  49%|████▉     | 47/96 [02:01<02:03,  2.52s/it, training_loss=0.090]\u001b[A\n",
      "Epoch 4:  50%|█████     | 48/96 [02:01<02:01,  2.53s/it, training_loss=0.090]\u001b[A\n",
      "Epoch 4:  50%|█████     | 48/96 [02:03<02:01,  2.53s/it, training_loss=0.139]\u001b[A\n",
      "Epoch 4:  51%|█████     | 49/96 [02:03<01:58,  2.53s/it, training_loss=0.139]\u001b[A\n",
      "Epoch 4:  51%|█████     | 49/96 [02:06<01:58,  2.53s/it, training_loss=0.174]\u001b[A\n",
      "Epoch 4:  52%|█████▏    | 50/96 [02:06<01:56,  2.53s/it, training_loss=0.174]\u001b[A\n",
      "Epoch 4:  52%|█████▏    | 50/96 [02:08<01:56,  2.53s/it, training_loss=0.143]\u001b[A\n",
      "Epoch 4:  53%|█████▎    | 51/96 [02:08<01:53,  2.52s/it, training_loss=0.143]\u001b[A\n",
      "Epoch 4:  53%|█████▎    | 51/96 [02:11<01:53,  2.52s/it, training_loss=0.179]\u001b[A\n",
      "Epoch 4:  54%|█████▍    | 52/96 [02:11<01:50,  2.52s/it, training_loss=0.179]\u001b[A\n",
      "Epoch 4:  54%|█████▍    | 52/96 [02:13<01:50,  2.52s/it, training_loss=0.143]\u001b[A\n",
      "Epoch 4:  55%|█████▌    | 53/96 [02:13<01:48,  2.52s/it, training_loss=0.143]\u001b[A\n",
      "Epoch 4:  55%|█████▌    | 53/96 [02:16<01:48,  2.52s/it, training_loss=0.082]\u001b[A\n",
      "Epoch 4:  56%|█████▋    | 54/96 [02:16<01:45,  2.52s/it, training_loss=0.082]\u001b[A\n",
      "Epoch 4:  56%|█████▋    | 54/96 [02:18<01:45,  2.52s/it, training_loss=0.141]\u001b[A\n",
      "Epoch 4:  57%|█████▋    | 55/96 [02:18<01:43,  2.52s/it, training_loss=0.141]\u001b[A\n",
      "Epoch 4:  57%|█████▋    | 55/96 [02:21<01:43,  2.52s/it, training_loss=0.108]\u001b[A\n",
      "Epoch 4:  58%|█████▊    | 56/96 [02:21<01:41,  2.53s/it, training_loss=0.108]\u001b[A\n",
      "Epoch 4:  58%|█████▊    | 56/96 [02:23<01:41,  2.53s/it, training_loss=0.088]\u001b[A\n",
      "Epoch 4:  59%|█████▉    | 57/96 [02:23<01:38,  2.53s/it, training_loss=0.088]\u001b[A\n",
      "Epoch 4:  59%|█████▉    | 57/96 [02:26<01:38,  2.53s/it, training_loss=0.158]\u001b[A\n",
      "Epoch 4:  60%|██████    | 58/96 [02:26<01:36,  2.53s/it, training_loss=0.158]\u001b[A\n",
      "Epoch 4:  60%|██████    | 58/96 [02:28<01:36,  2.53s/it, training_loss=0.196]\u001b[A\n",
      "Epoch 4:  61%|██████▏   | 59/96 [02:28<01:33,  2.53s/it, training_loss=0.196]\u001b[A\n",
      "Epoch 4:  61%|██████▏   | 59/96 [02:31<01:33,  2.53s/it, training_loss=0.162]\u001b[A\n",
      "Epoch 4:  62%|██████▎   | 60/96 [02:31<01:30,  2.53s/it, training_loss=0.162]\u001b[A\n",
      "Epoch 4:  62%|██████▎   | 60/96 [02:33<01:30,  2.53s/it, training_loss=0.112]\u001b[A\n",
      "Epoch 4:  64%|██████▎   | 61/96 [02:33<01:28,  2.52s/it, training_loss=0.112]\u001b[A\n",
      "Epoch 4:  64%|██████▎   | 61/96 [02:36<01:28,  2.52s/it, training_loss=0.097]\u001b[A\n",
      "Epoch 4:  65%|██████▍   | 62/96 [02:36<01:25,  2.52s/it, training_loss=0.097]\u001b[A\n",
      "Epoch 4:  65%|██████▍   | 62/96 [02:38<01:25,  2.52s/it, training_loss=0.125]\u001b[A\n",
      "Epoch 4:  66%|██████▌   | 63/96 [02:38<01:23,  2.52s/it, training_loss=0.125]\u001b[A\n",
      "Epoch 4:  66%|██████▌   | 63/96 [02:41<01:23,  2.52s/it, training_loss=0.100]\u001b[A\n",
      "Epoch 4:  67%|██████▋   | 64/96 [02:41<01:20,  2.52s/it, training_loss=0.100]\u001b[A\n",
      "Epoch 4:  67%|██████▋   | 64/96 [02:43<01:20,  2.52s/it, training_loss=0.156]\u001b[A\n",
      "Epoch 4:  68%|██████▊   | 65/96 [02:43<01:17,  2.51s/it, training_loss=0.156]\u001b[A\n",
      "Epoch 4:  68%|██████▊   | 65/96 [02:46<01:17,  2.51s/it, training_loss=0.132]\u001b[A\n",
      "Epoch 4:  69%|██████▉   | 66/96 [02:46<01:15,  2.51s/it, training_loss=0.132]\u001b[A\n",
      "Epoch 4:  69%|██████▉   | 66/96 [02:49<01:15,  2.51s/it, training_loss=0.318]\u001b[A\n",
      "Epoch 4:  70%|██████▉   | 67/96 [02:49<01:12,  2.51s/it, training_loss=0.318]\u001b[A\n",
      "Epoch 4:  70%|██████▉   | 67/96 [02:51<01:12,  2.51s/it, training_loss=0.150]\u001b[A\n",
      "Epoch 4:  71%|███████   | 68/96 [02:51<01:10,  2.51s/it, training_loss=0.150]\u001b[A\n",
      "Epoch 4:  71%|███████   | 68/96 [02:54<01:10,  2.51s/it, training_loss=0.118]\u001b[A\n",
      "Epoch 4:  72%|███████▏  | 69/96 [02:54<01:08,  2.52s/it, training_loss=0.118]\u001b[A\n",
      "Epoch 4:  72%|███████▏  | 69/96 [02:56<01:08,  2.52s/it, training_loss=0.143]\u001b[A\n",
      "Epoch 4:  73%|███████▎  | 70/96 [02:56<01:05,  2.52s/it, training_loss=0.143]\u001b[A\n",
      "Epoch 4:  73%|███████▎  | 70/96 [02:59<01:05,  2.52s/it, training_loss=0.157]\u001b[A\n",
      "Epoch 4:  74%|███████▍  | 71/96 [02:59<01:03,  2.52s/it, training_loss=0.157]\u001b[A\n",
      "Epoch 4:  74%|███████▍  | 71/96 [03:01<01:03,  2.52s/it, training_loss=0.135]\u001b[A\n",
      "Epoch 4:  75%|███████▌  | 72/96 [03:01<01:00,  2.52s/it, training_loss=0.135]\u001b[A\n",
      "Epoch 4:  75%|███████▌  | 72/96 [03:04<01:00,  2.52s/it, training_loss=0.105]\u001b[A\n",
      "Epoch 4:  76%|███████▌  | 73/96 [03:04<00:57,  2.52s/it, training_loss=0.105]\u001b[A\n",
      "Epoch 4:  76%|███████▌  | 73/96 [03:06<00:57,  2.52s/it, training_loss=0.114]\u001b[A\n",
      "Epoch 4:  77%|███████▋  | 74/96 [03:06<00:55,  2.52s/it, training_loss=0.114]\u001b[A\n",
      "Epoch 4:  77%|███████▋  | 74/96 [03:09<00:55,  2.52s/it, training_loss=0.133]\u001b[A\n",
      "Epoch 4:  78%|███████▊  | 75/96 [03:09<00:52,  2.52s/it, training_loss=0.133]\u001b[A\n",
      "Epoch 4:  78%|███████▊  | 75/96 [03:11<00:52,  2.52s/it, training_loss=0.151]\u001b[A\n",
      "Epoch 4:  79%|███████▉  | 76/96 [03:11<00:50,  2.52s/it, training_loss=0.151]\u001b[A\n",
      "Epoch 4:  79%|███████▉  | 76/96 [03:14<00:50,  2.52s/it, training_loss=0.149]\u001b[A\n",
      "Epoch 4:  80%|████████  | 77/96 [03:14<00:47,  2.52s/it, training_loss=0.149]\u001b[A\n",
      "Epoch 4:  80%|████████  | 77/96 [03:16<00:47,  2.52s/it, training_loss=0.127]\u001b[A\n",
      "Epoch 4:  81%|████████▏ | 78/96 [03:16<00:45,  2.52s/it, training_loss=0.127]\u001b[A\n",
      "Epoch 4:  81%|████████▏ | 78/96 [03:19<00:45,  2.52s/it, training_loss=0.074]\u001b[A\n",
      "Epoch 4:  82%|████████▏ | 79/96 [03:19<00:42,  2.52s/it, training_loss=0.074]\u001b[A\n",
      "Epoch 4:  82%|████████▏ | 79/96 [03:21<00:42,  2.52s/it, training_loss=0.085]\u001b[A\n",
      "Epoch 4:  83%|████████▎ | 80/96 [03:21<00:40,  2.51s/it, training_loss=0.085]\u001b[A\n",
      "Epoch 4:  83%|████████▎ | 80/96 [03:24<00:40,  2.51s/it, training_loss=0.161]\u001b[A\n",
      "Epoch 4:  84%|████████▍ | 81/96 [03:24<00:37,  2.51s/it, training_loss=0.161]\u001b[A\n",
      "Epoch 4:  84%|████████▍ | 81/96 [03:26<00:37,  2.51s/it, training_loss=0.123]\u001b[A\n",
      "Epoch 4:  85%|████████▌ | 82/96 [03:26<00:35,  2.51s/it, training_loss=0.123]\u001b[A\n",
      "Epoch 4:  85%|████████▌ | 82/96 [03:29<00:35,  2.51s/it, training_loss=0.137]\u001b[A\n",
      "Epoch 4:  86%|████████▋ | 83/96 [03:29<00:32,  2.51s/it, training_loss=0.137]\u001b[A\n",
      "Epoch 4:  86%|████████▋ | 83/96 [03:31<00:32,  2.51s/it, training_loss=0.123]\u001b[A\n",
      "Epoch 4:  88%|████████▊ | 84/96 [03:31<00:30,  2.52s/it, training_loss=0.123]\u001b[A\n",
      "Epoch 4:  88%|████████▊ | 84/96 [03:34<00:30,  2.52s/it, training_loss=0.101]\u001b[A\n",
      "Epoch 4:  89%|████████▊ | 85/96 [03:34<00:27,  2.52s/it, training_loss=0.101]\u001b[A\n",
      "Epoch 4:  89%|████████▊ | 85/96 [03:36<00:27,  2.52s/it, training_loss=0.136]\u001b[A\n",
      "Epoch 4:  90%|████████▉ | 86/96 [03:36<00:25,  2.52s/it, training_loss=0.136]\u001b[A\n",
      "Epoch 4:  90%|████████▉ | 86/96 [03:39<00:25,  2.52s/it, training_loss=0.199]\u001b[A\n",
      "Epoch 4:  91%|█████████ | 87/96 [03:39<00:22,  2.52s/it, training_loss=0.199]\u001b[A\n",
      "Epoch 4:  91%|█████████ | 87/96 [03:41<00:22,  2.52s/it, training_loss=0.091]\u001b[A\n",
      "Epoch 4:  92%|█████████▏| 88/96 [03:41<00:20,  2.52s/it, training_loss=0.091]\u001b[A\n",
      "Epoch 4:  92%|█████████▏| 88/96 [03:44<00:20,  2.52s/it, training_loss=0.152]\u001b[A\n",
      "Epoch 4:  93%|█████████▎| 89/96 [03:44<00:17,  2.53s/it, training_loss=0.152]\u001b[A\n",
      "Epoch 4:  93%|█████████▎| 89/96 [03:46<00:17,  2.53s/it, training_loss=0.156]\u001b[A\n",
      "Epoch 4:  94%|█████████▍| 90/96 [03:47<00:15,  2.53s/it, training_loss=0.156]\u001b[A\n",
      "Epoch 4:  94%|█████████▍| 90/96 [03:49<00:15,  2.53s/it, training_loss=0.103]\u001b[A\n",
      "Epoch 4:  95%|█████████▍| 91/96 [03:49<00:12,  2.53s/it, training_loss=0.103]\u001b[A\n",
      "Epoch 4:  95%|█████████▍| 91/96 [03:52<00:12,  2.53s/it, training_loss=0.096]\u001b[A\n",
      "Epoch 4:  96%|█████████▌| 92/96 [03:52<00:10,  2.53s/it, training_loss=0.096]\u001b[A\n",
      "Epoch 4:  96%|█████████▌| 92/96 [03:54<00:10,  2.53s/it, training_loss=0.146]\u001b[A\n",
      "Epoch 4:  97%|█████████▋| 93/96 [03:54<00:07,  2.53s/it, training_loss=0.146]\u001b[A\n",
      "Epoch 4:  97%|█████████▋| 93/96 [03:57<00:07,  2.53s/it, training_loss=0.094]\u001b[A\n",
      "Epoch 4:  98%|█████████▊| 94/96 [03:57<00:05,  2.53s/it, training_loss=0.094]\u001b[A\n",
      "Epoch 4:  98%|█████████▊| 94/96 [03:59<00:05,  2.53s/it, training_loss=0.307]\u001b[A\n",
      "Epoch 4:  99%|█████████▉| 95/96 [03:59<00:02,  2.53s/it, training_loss=0.307]\u001b[A\n",
      "Epoch 4:  99%|█████████▉| 95/96 [04:01<00:02,  2.53s/it, training_loss=0.140]\u001b[A\n",
      "Epoch 4: 100%|██████████| 96/96 [04:01<00:00,  2.26s/it, training_loss=0.140]\u001b[A\n",
      " 60%|██████    | 3/5 [16:54<08:34, 257.05s/it]\n",
      "  0%|          | 0/17 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4\n",
      "Training loss: 0.41105999595796067\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  6%|▌         | 1/17 [00:00<00:14,  1.08it/s]\u001b[A\n",
      " 12%|█▏        | 2/17 [00:01<00:13,  1.08it/s]\u001b[A\n",
      " 18%|█▊        | 3/17 [00:02<00:12,  1.08it/s]\u001b[A\n",
      " 24%|██▎       | 4/17 [00:03<00:11,  1.09it/s]\u001b[A\n",
      " 29%|██▉       | 5/17 [00:04<00:11,  1.09it/s]\u001b[A\n",
      " 35%|███▌      | 6/17 [00:05<00:10,  1.09it/s]\u001b[A\n",
      " 41%|████      | 7/17 [00:06<00:09,  1.09it/s]\u001b[A\n",
      " 47%|████▋     | 8/17 [00:07<00:08,  1.09it/s]\u001b[A\n",
      " 53%|█████▎    | 9/17 [00:08<00:07,  1.09it/s]\u001b[A\n",
      " 59%|█████▉    | 10/17 [00:09<00:06,  1.09it/s]\u001b[A\n",
      " 65%|██████▍   | 11/17 [00:10<00:05,  1.09it/s]\u001b[A\n",
      " 71%|███████   | 12/17 [00:11<00:04,  1.09it/s]\u001b[A\n",
      " 76%|███████▋  | 13/17 [00:11<00:03,  1.08it/s]\u001b[A\n",
      " 82%|████████▏ | 14/17 [00:12<00:02,  1.08it/s]\u001b[A\n",
      " 88%|████████▊ | 15/17 [00:13<00:01,  1.08it/s]\u001b[A\n",
      " 94%|█████████▍| 16/17 [00:14<00:00,  1.08it/s]\u001b[A\n",
      "100%|██████████| 17/17 [00:15<00:00,  1.09it/s]\n",
      " 80%|████████  | 4/5 [17:10<04:17, 257.47s/it]\n",
      "Epoch 5:   0%|          | 0/96 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.47325433878337636\n",
      "F1 Score (Weighted): 0.8812405608892372\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5:   0%|          | 0/96 [00:02<?, ?it/s, training_loss=0.131]\u001b[A\n",
      "Epoch 5:   1%|          | 1/96 [00:02<03:58,  2.51s/it, training_loss=0.131]\u001b[A\n",
      "Epoch 5:   1%|          | 1/96 [00:05<03:58,  2.51s/it, training_loss=0.122]\u001b[A\n",
      "Epoch 5:   2%|▏         | 2/96 [00:05<03:56,  2.51s/it, training_loss=0.122]\u001b[A\n",
      "Epoch 5:   2%|▏         | 2/96 [00:07<03:56,  2.51s/it, training_loss=0.060]\u001b[A\n",
      "Epoch 5:   3%|▎         | 3/96 [00:07<03:53,  2.51s/it, training_loss=0.060]\u001b[A\n",
      "Epoch 5:   3%|▎         | 3/96 [00:10<03:53,  2.51s/it, training_loss=0.087]\u001b[A\n",
      "Epoch 5:   4%|▍         | 4/96 [00:10<03:51,  2.52s/it, training_loss=0.087]\u001b[A\n",
      "Epoch 5:   4%|▍         | 4/96 [00:12<03:51,  2.52s/it, training_loss=0.108]\u001b[A\n",
      "Epoch 5:   5%|▌         | 5/96 [00:12<03:49,  2.52s/it, training_loss=0.108]\u001b[A\n",
      "Epoch 5:   5%|▌         | 5/96 [00:15<03:49,  2.52s/it, training_loss=0.108]\u001b[A\n",
      "Epoch 5:   6%|▋         | 6/96 [00:15<03:47,  2.52s/it, training_loss=0.108]\u001b[A\n",
      "Epoch 5:   6%|▋         | 6/96 [00:17<03:47,  2.52s/it, training_loss=0.089]\u001b[A\n",
      "Epoch 5:   7%|▋         | 7/96 [00:17<03:44,  2.52s/it, training_loss=0.089]\u001b[A\n",
      "Epoch 5:   7%|▋         | 7/96 [00:20<03:44,  2.52s/it, training_loss=0.074]\u001b[A\n",
      "Epoch 5:   8%|▊         | 8/96 [00:20<03:41,  2.52s/it, training_loss=0.074]\u001b[A\n",
      "Epoch 5:   8%|▊         | 8/96 [00:22<03:41,  2.52s/it, training_loss=0.126]\u001b[A\n",
      "Epoch 5:   9%|▉         | 9/96 [00:22<03:39,  2.52s/it, training_loss=0.126]\u001b[A\n",
      "Epoch 5:   9%|▉         | 9/96 [00:25<03:39,  2.52s/it, training_loss=0.076]\u001b[A\n",
      "Epoch 5:  10%|█         | 10/96 [00:25<03:36,  2.52s/it, training_loss=0.076]\u001b[A\n",
      "Epoch 5:  10%|█         | 10/96 [00:27<03:36,  2.52s/it, training_loss=0.093]\u001b[A\n",
      "Epoch 5:  11%|█▏        | 11/96 [00:27<03:34,  2.52s/it, training_loss=0.093]\u001b[A\n",
      "Epoch 5:  11%|█▏        | 11/96 [00:30<03:34,  2.52s/it, training_loss=0.106]\u001b[A\n",
      "Epoch 5:  12%|█▎        | 12/96 [00:30<03:31,  2.52s/it, training_loss=0.106]\u001b[A\n",
      "Epoch 5:  12%|█▎        | 12/96 [00:32<03:31,  2.52s/it, training_loss=0.078]\u001b[A\n",
      "Epoch 5:  14%|█▎        | 13/96 [00:32<03:29,  2.52s/it, training_loss=0.078]\u001b[A\n",
      "Epoch 5:  14%|█▎        | 13/96 [00:35<03:29,  2.52s/it, training_loss=0.305]\u001b[A\n",
      "Epoch 5:  15%|█▍        | 14/96 [00:35<03:26,  2.51s/it, training_loss=0.305]\u001b[A\n",
      "Epoch 5:  15%|█▍        | 14/96 [00:37<03:26,  2.51s/it, training_loss=0.156]\u001b[A\n",
      "Epoch 5:  16%|█▌        | 15/96 [00:37<03:23,  2.51s/it, training_loss=0.156]\u001b[A\n",
      "Epoch 5:  16%|█▌        | 15/96 [00:40<03:23,  2.51s/it, training_loss=0.188]\u001b[A\n",
      "Epoch 5:  17%|█▋        | 16/96 [00:40<03:21,  2.51s/it, training_loss=0.188]\u001b[A\n",
      "Epoch 5:  17%|█▋        | 16/96 [00:42<03:21,  2.51s/it, training_loss=0.090]\u001b[A\n",
      "Epoch 5:  18%|█▊        | 17/96 [00:42<03:18,  2.52s/it, training_loss=0.090]\u001b[A\n",
      "Epoch 5:  18%|█▊        | 17/96 [00:45<03:18,  2.52s/it, training_loss=0.135]\u001b[A\n",
      "Epoch 5:  19%|█▉        | 18/96 [00:45<03:16,  2.52s/it, training_loss=0.135]\u001b[A\n",
      "Epoch 5:  19%|█▉        | 18/96 [00:47<03:16,  2.52s/it, training_loss=0.179]\u001b[A\n",
      "Epoch 5:  20%|█▉        | 19/96 [00:47<03:13,  2.52s/it, training_loss=0.179]\u001b[A\n",
      "Epoch 5:  20%|█▉        | 19/96 [00:50<03:13,  2.52s/it, training_loss=0.094]\u001b[A\n",
      "Epoch 5:  21%|██        | 20/96 [00:50<03:11,  2.52s/it, training_loss=0.094]\u001b[A\n",
      "Epoch 5:  21%|██        | 20/96 [00:52<03:11,  2.52s/it, training_loss=0.155]\u001b[A\n",
      "Epoch 5:  22%|██▏       | 21/96 [00:52<03:09,  2.53s/it, training_loss=0.155]\u001b[A\n",
      "Epoch 5:  22%|██▏       | 21/96 [00:55<03:09,  2.53s/it, training_loss=0.119]\u001b[A\n",
      "Epoch 5:  23%|██▎       | 22/96 [00:55<03:06,  2.52s/it, training_loss=0.119]\u001b[A\n",
      "Epoch 5:  23%|██▎       | 22/96 [00:57<03:06,  2.52s/it, training_loss=0.127]\u001b[A\n",
      "Epoch 5:  24%|██▍       | 23/96 [00:57<03:04,  2.52s/it, training_loss=0.127]\u001b[A\n",
      "Epoch 5:  24%|██▍       | 23/96 [01:00<03:04,  2.52s/it, training_loss=0.191]\u001b[A\n",
      "Epoch 5:  25%|██▌       | 24/96 [01:00<03:01,  2.52s/it, training_loss=0.191]\u001b[A\n",
      "Epoch 5:  25%|██▌       | 24/96 [01:02<03:01,  2.52s/it, training_loss=0.133]\u001b[A\n",
      "Epoch 5:  26%|██▌       | 25/96 [01:03<02:59,  2.52s/it, training_loss=0.133]\u001b[A\n",
      "Epoch 5:  26%|██▌       | 25/96 [01:05<02:59,  2.52s/it, training_loss=0.064]\u001b[A\n",
      "Epoch 5:  27%|██▋       | 26/96 [01:05<02:56,  2.53s/it, training_loss=0.064]\u001b[A\n",
      "Epoch 5:  27%|██▋       | 26/96 [01:08<02:56,  2.53s/it, training_loss=0.178]\u001b[A\n",
      "Epoch 5:  28%|██▊       | 27/96 [01:08<02:54,  2.53s/it, training_loss=0.178]\u001b[A\n",
      "Epoch 5:  28%|██▊       | 27/96 [01:10<02:54,  2.53s/it, training_loss=0.139]\u001b[A\n",
      "Epoch 5:  29%|██▉       | 28/96 [01:10<02:52,  2.53s/it, training_loss=0.139]\u001b[A\n",
      "Epoch 5:  29%|██▉       | 28/96 [01:13<02:52,  2.53s/it, training_loss=0.085]\u001b[A\n",
      "Epoch 5:  30%|███       | 29/96 [01:13<02:49,  2.52s/it, training_loss=0.085]\u001b[A\n",
      "Epoch 5:  30%|███       | 29/96 [01:15<02:49,  2.52s/it, training_loss=0.092]\u001b[A\n",
      "Epoch 5:  31%|███▏      | 30/96 [01:15<02:46,  2.53s/it, training_loss=0.092]\u001b[A\n",
      "Epoch 5:  31%|███▏      | 30/96 [01:18<02:46,  2.53s/it, training_loss=0.118]\u001b[A\n",
      "Epoch 5:  32%|███▏      | 31/96 [01:18<02:44,  2.53s/it, training_loss=0.118]\u001b[A\n",
      "Epoch 5:  32%|███▏      | 31/96 [01:20<02:44,  2.53s/it, training_loss=0.086]\u001b[A\n",
      "Epoch 5:  33%|███▎      | 32/96 [01:20<02:41,  2.53s/it, training_loss=0.086]\u001b[A\n",
      "Epoch 5:  33%|███▎      | 32/96 [01:23<02:41,  2.53s/it, training_loss=0.139]\u001b[A\n",
      "Epoch 5:  34%|███▍      | 33/96 [01:23<02:38,  2.52s/it, training_loss=0.139]\u001b[A\n",
      "Epoch 5:  34%|███▍      | 33/96 [01:25<02:38,  2.52s/it, training_loss=0.150]\u001b[A\n",
      "Epoch 5:  35%|███▌      | 34/96 [01:25<02:36,  2.52s/it, training_loss=0.150]\u001b[A\n",
      "Epoch 5:  35%|███▌      | 34/96 [01:28<02:36,  2.52s/it, training_loss=0.154]\u001b[A\n",
      "Epoch 5:  36%|███▋      | 35/96 [01:28<02:33,  2.52s/it, training_loss=0.154]\u001b[A\n",
      "Epoch 5:  36%|███▋      | 35/96 [01:30<02:33,  2.52s/it, training_loss=0.183]\u001b[A\n",
      "Epoch 5:  38%|███▊      | 36/96 [01:30<02:31,  2.52s/it, training_loss=0.183]\u001b[A\n",
      "Epoch 5:  38%|███▊      | 36/96 [01:33<02:31,  2.52s/it, training_loss=0.052]\u001b[A\n",
      "Epoch 5:  39%|███▊      | 37/96 [01:33<02:28,  2.52s/it, training_loss=0.052]\u001b[A\n",
      "Epoch 5:  39%|███▊      | 37/96 [01:35<02:28,  2.52s/it, training_loss=0.141]\u001b[A\n",
      "Epoch 5:  40%|███▉      | 38/96 [01:35<02:26,  2.52s/it, training_loss=0.141]\u001b[A\n",
      "Epoch 5:  40%|███▉      | 38/96 [01:38<02:26,  2.52s/it, training_loss=0.102]\u001b[A\n",
      "Epoch 5:  41%|████      | 39/96 [01:38<02:23,  2.52s/it, training_loss=0.102]\u001b[A\n",
      "Epoch 5:  41%|████      | 39/96 [01:40<02:23,  2.52s/it, training_loss=0.116]\u001b[A\n",
      "Epoch 5:  42%|████▏     | 40/96 [01:40<02:20,  2.52s/it, training_loss=0.116]\u001b[A\n",
      "Epoch 5:  42%|████▏     | 40/96 [01:43<02:20,  2.52s/it, training_loss=0.136]\u001b[A\n",
      "Epoch 5:  43%|████▎     | 41/96 [01:43<02:18,  2.52s/it, training_loss=0.136]\u001b[A\n",
      "Epoch 5:  43%|████▎     | 41/96 [01:45<02:18,  2.52s/it, training_loss=0.092]\u001b[A\n",
      "Epoch 5:  44%|████▍     | 42/96 [01:45<02:15,  2.52s/it, training_loss=0.092]\u001b[A\n",
      "Epoch 5:  44%|████▍     | 42/96 [01:48<02:15,  2.52s/it, training_loss=0.109]\u001b[A\n",
      "Epoch 5:  45%|████▍     | 43/96 [01:48<02:13,  2.52s/it, training_loss=0.109]\u001b[A\n",
      "Epoch 5:  45%|████▍     | 43/96 [01:50<02:13,  2.52s/it, training_loss=0.103]\u001b[A\n",
      "Epoch 5:  46%|████▌     | 44/96 [01:50<02:10,  2.52s/it, training_loss=0.103]\u001b[A\n",
      "Epoch 5:  46%|████▌     | 44/96 [01:53<02:10,  2.52s/it, training_loss=0.178]\u001b[A\n",
      "Epoch 5:  47%|████▋     | 45/96 [01:53<02:08,  2.52s/it, training_loss=0.178]\u001b[A\n",
      "Epoch 5:  47%|████▋     | 45/96 [01:55<02:08,  2.52s/it, training_loss=0.118]\u001b[A\n",
      "Epoch 5:  48%|████▊     | 46/96 [01:55<02:05,  2.52s/it, training_loss=0.118]\u001b[A\n",
      "Epoch 5:  48%|████▊     | 46/96 [01:58<02:05,  2.52s/it, training_loss=0.139]\u001b[A\n",
      "Epoch 5:  49%|████▉     | 47/96 [01:58<02:03,  2.52s/it, training_loss=0.139]\u001b[A\n",
      "Epoch 5:  49%|████▉     | 47/96 [02:00<02:03,  2.52s/it, training_loss=0.126]\u001b[A\n",
      "Epoch 5:  50%|█████     | 48/96 [02:00<02:00,  2.52s/it, training_loss=0.126]\u001b[A\n",
      "Epoch 5:  50%|█████     | 48/96 [02:03<02:00,  2.52s/it, training_loss=0.103]\u001b[A\n",
      "Epoch 5:  51%|█████     | 49/96 [02:03<01:58,  2.52s/it, training_loss=0.103]\u001b[A\n",
      "Epoch 5:  51%|█████     | 49/96 [02:06<01:58,  2.52s/it, training_loss=0.077]\u001b[A\n",
      "Epoch 5:  52%|█████▏    | 50/96 [02:06<01:55,  2.52s/it, training_loss=0.077]\u001b[A\n",
      "Epoch 5:  52%|█████▏    | 50/96 [02:08<01:55,  2.52s/it, training_loss=0.129]\u001b[A\n",
      "Epoch 5:  53%|█████▎    | 51/96 [02:08<01:53,  2.52s/it, training_loss=0.129]\u001b[A\n",
      "Epoch 5:  53%|█████▎    | 51/96 [02:11<01:53,  2.52s/it, training_loss=0.096]\u001b[A\n",
      "Epoch 5:  54%|█████▍    | 52/96 [02:11<01:51,  2.52s/it, training_loss=0.096]\u001b[A\n",
      "Epoch 5:  54%|█████▍    | 52/96 [02:13<01:51,  2.52s/it, training_loss=0.101]\u001b[A\n",
      "Epoch 5:  55%|█████▌    | 53/96 [02:13<01:48,  2.53s/it, training_loss=0.101]\u001b[A\n",
      "Epoch 5:  55%|█████▌    | 53/96 [02:16<01:48,  2.53s/it, training_loss=0.111]\u001b[A\n",
      "Epoch 5:  56%|█████▋    | 54/96 [02:16<01:46,  2.53s/it, training_loss=0.111]\u001b[A\n",
      "Epoch 5:  56%|█████▋    | 54/96 [02:18<01:46,  2.53s/it, training_loss=0.156]\u001b[A\n",
      "Epoch 5:  57%|█████▋    | 55/96 [02:18<01:43,  2.53s/it, training_loss=0.156]\u001b[A\n",
      "Epoch 5:  57%|█████▋    | 55/96 [02:21<01:43,  2.53s/it, training_loss=0.106]\u001b[A\n",
      "Epoch 5:  58%|█████▊    | 56/96 [02:21<01:41,  2.53s/it, training_loss=0.106]\u001b[A\n",
      "Epoch 5:  58%|█████▊    | 56/96 [02:23<01:41,  2.53s/it, training_loss=0.109]\u001b[A\n",
      "Epoch 5:  59%|█████▉    | 57/96 [02:23<01:38,  2.52s/it, training_loss=0.109]\u001b[A\n",
      "Epoch 5:  59%|█████▉    | 57/96 [02:26<01:38,  2.52s/it, training_loss=0.080]\u001b[A\n",
      "Epoch 5:  60%|██████    | 58/96 [02:26<01:35,  2.52s/it, training_loss=0.080]\u001b[A\n",
      "Epoch 5:  60%|██████    | 58/96 [02:28<01:35,  2.52s/it, training_loss=0.120]\u001b[A\n",
      "Epoch 5:  61%|██████▏   | 59/96 [02:28<01:33,  2.53s/it, training_loss=0.120]\u001b[A\n",
      "Epoch 5:  61%|██████▏   | 59/96 [02:31<01:33,  2.53s/it, training_loss=0.125]\u001b[A\n",
      "Epoch 5:  62%|██████▎   | 60/96 [02:31<01:31,  2.53s/it, training_loss=0.125]\u001b[A\n",
      "Epoch 5:  62%|██████▎   | 60/96 [02:33<01:31,  2.53s/it, training_loss=0.090]\u001b[A\n",
      "Epoch 5:  64%|██████▎   | 61/96 [02:33<01:28,  2.53s/it, training_loss=0.090]\u001b[A\n",
      "Epoch 5:  64%|██████▎   | 61/96 [02:36<01:28,  2.53s/it, training_loss=0.089]\u001b[A\n",
      "Epoch 5:  65%|██████▍   | 62/96 [02:36<01:26,  2.53s/it, training_loss=0.089]\u001b[A\n",
      "Epoch 5:  65%|██████▍   | 62/96 [02:38<01:26,  2.53s/it, training_loss=0.086]\u001b[A\n",
      "Epoch 5:  66%|██████▌   | 63/96 [02:38<01:23,  2.53s/it, training_loss=0.086]\u001b[A\n",
      "Epoch 5:  66%|██████▌   | 63/96 [02:41<01:23,  2.53s/it, training_loss=0.099]\u001b[A\n",
      "Epoch 5:  67%|██████▋   | 64/96 [02:41<01:20,  2.53s/it, training_loss=0.099]\u001b[A\n",
      "Epoch 5:  67%|██████▋   | 64/96 [02:43<01:20,  2.53s/it, training_loss=0.144]\u001b[A\n",
      "Epoch 5:  68%|██████▊   | 65/96 [02:43<01:18,  2.53s/it, training_loss=0.144]\u001b[A\n",
      "Epoch 5:  68%|██████▊   | 65/96 [02:46<01:18,  2.53s/it, training_loss=0.118]\u001b[A\n",
      "Epoch 5:  69%|██████▉   | 66/96 [02:46<01:15,  2.52s/it, training_loss=0.118]\u001b[A\n",
      "Epoch 5:  69%|██████▉   | 66/96 [02:48<01:15,  2.52s/it, training_loss=0.253]\u001b[A\n",
      "Epoch 5:  70%|██████▉   | 67/96 [02:48<01:13,  2.52s/it, training_loss=0.253]\u001b[A\n",
      "Epoch 5:  70%|██████▉   | 67/96 [02:51<01:13,  2.52s/it, training_loss=0.130]\u001b[A\n",
      "Epoch 5:  71%|███████   | 68/96 [02:51<01:10,  2.52s/it, training_loss=0.130]\u001b[A\n",
      "Epoch 5:  71%|███████   | 68/96 [02:54<01:10,  2.52s/it, training_loss=0.100]\u001b[A\n",
      "Epoch 5:  72%|███████▏  | 69/96 [02:54<01:08,  2.52s/it, training_loss=0.100]\u001b[A\n",
      "Epoch 5:  72%|███████▏  | 69/96 [02:56<01:08,  2.52s/it, training_loss=0.155]\u001b[A\n",
      "Epoch 5:  73%|███████▎  | 70/96 [02:56<01:05,  2.52s/it, training_loss=0.155]\u001b[A\n",
      "Epoch 5:  73%|███████▎  | 70/96 [02:59<01:05,  2.52s/it, training_loss=0.067]\u001b[A\n",
      "Epoch 5:  74%|███████▍  | 71/96 [02:59<01:03,  2.53s/it, training_loss=0.067]\u001b[A\n",
      "Epoch 5:  74%|███████▍  | 71/96 [03:01<01:03,  2.53s/it, training_loss=0.078]\u001b[A\n",
      "Epoch 5:  75%|███████▌  | 72/96 [03:01<01:00,  2.53s/it, training_loss=0.078]\u001b[A\n",
      "Epoch 5:  75%|███████▌  | 72/96 [03:04<01:00,  2.53s/it, training_loss=0.107]\u001b[A\n",
      "Epoch 5:  76%|███████▌  | 73/96 [03:04<00:58,  2.53s/it, training_loss=0.107]\u001b[A\n",
      "Epoch 5:  76%|███████▌  | 73/96 [03:06<00:58,  2.53s/it, training_loss=0.139]\u001b[A\n",
      "Epoch 5:  77%|███████▋  | 74/96 [03:06<00:55,  2.53s/it, training_loss=0.139]\u001b[A\n",
      "Epoch 5:  77%|███████▋  | 74/96 [03:09<00:55,  2.53s/it, training_loss=0.067]\u001b[A\n",
      "Epoch 5:  78%|███████▊  | 75/96 [03:09<00:53,  2.53s/it, training_loss=0.067]\u001b[A\n",
      "Epoch 5:  78%|███████▊  | 75/96 [03:11<00:53,  2.53s/it, training_loss=0.127]\u001b[A\n",
      "Epoch 5:  79%|███████▉  | 76/96 [03:11<00:50,  2.53s/it, training_loss=0.127]\u001b[A\n",
      "Epoch 5:  79%|███████▉  | 76/96 [03:14<00:50,  2.53s/it, training_loss=0.076]\u001b[A\n",
      "Epoch 5:  80%|████████  | 77/96 [03:14<00:48,  2.53s/it, training_loss=0.076]\u001b[A\n",
      "Epoch 5:  80%|████████  | 77/96 [03:16<00:48,  2.53s/it, training_loss=0.061]\u001b[A\n",
      "Epoch 5:  81%|████████▏ | 78/96 [03:16<00:45,  2.53s/it, training_loss=0.061]\u001b[A\n",
      "Epoch 5:  81%|████████▏ | 78/96 [03:19<00:45,  2.53s/it, training_loss=0.084]\u001b[A\n",
      "Epoch 5:  82%|████████▏ | 79/96 [03:19<00:42,  2.52s/it, training_loss=0.084]\u001b[A\n",
      "Epoch 5:  82%|████████▏ | 79/96 [03:21<00:42,  2.52s/it, training_loss=0.081]\u001b[A\n",
      "Epoch 5:  83%|████████▎ | 80/96 [03:21<00:40,  2.52s/it, training_loss=0.081]\u001b[A\n",
      "Epoch 5:  83%|████████▎ | 80/96 [03:24<00:40,  2.52s/it, training_loss=0.088]\u001b[A\n",
      "Epoch 5:  84%|████████▍ | 81/96 [03:24<00:37,  2.52s/it, training_loss=0.088]\u001b[A\n",
      "Epoch 5:  84%|████████▍ | 81/96 [03:26<00:37,  2.52s/it, training_loss=0.168]\u001b[A\n",
      "Epoch 5:  85%|████████▌ | 82/96 [03:26<00:35,  2.52s/it, training_loss=0.168]\u001b[A\n",
      "Epoch 5:  85%|████████▌ | 82/96 [03:29<00:35,  2.52s/it, training_loss=0.118]\u001b[A\n",
      "Epoch 5:  86%|████████▋ | 83/96 [03:29<00:32,  2.52s/it, training_loss=0.118]\u001b[A\n",
      "Epoch 5:  86%|████████▋ | 83/96 [03:31<00:32,  2.52s/it, training_loss=0.118]\u001b[A\n",
      "Epoch 5:  88%|████████▊ | 84/96 [03:31<00:30,  2.52s/it, training_loss=0.118]\u001b[A\n",
      "Epoch 5:  88%|████████▊ | 84/96 [03:34<00:30,  2.52s/it, training_loss=0.102]\u001b[A\n",
      "Epoch 5:  89%|████████▊ | 85/96 [03:34<00:27,  2.52s/it, training_loss=0.102]\u001b[A\n",
      "Epoch 5:  89%|████████▊ | 85/96 [03:36<00:27,  2.52s/it, training_loss=0.149]\u001b[A\n",
      "Epoch 5:  90%|████████▉ | 86/96 [03:36<00:25,  2.53s/it, training_loss=0.149]\u001b[A\n",
      "Epoch 5:  90%|████████▉ | 86/96 [03:39<00:25,  2.53s/it, training_loss=0.124]\u001b[A\n",
      "Epoch 5:  91%|█████████ | 87/96 [03:39<00:22,  2.53s/it, training_loss=0.124]\u001b[A\n",
      "Epoch 5:  91%|█████████ | 87/96 [03:42<00:22,  2.53s/it, training_loss=0.104]\u001b[A\n",
      "Epoch 5:  92%|█████████▏| 88/96 [03:42<00:20,  2.53s/it, training_loss=0.104]\u001b[A\n",
      "Epoch 5:  92%|█████████▏| 88/96 [03:44<00:20,  2.53s/it, training_loss=0.103]\u001b[A\n",
      "Epoch 5:  93%|█████████▎| 89/96 [03:44<00:17,  2.52s/it, training_loss=0.103]\u001b[A\n",
      "Epoch 5:  93%|█████████▎| 89/96 [03:47<00:17,  2.52s/it, training_loss=0.102]\u001b[A\n",
      "Epoch 5:  94%|█████████▍| 90/96 [03:47<00:15,  2.52s/it, training_loss=0.102]\u001b[A\n",
      "Epoch 5:  94%|█████████▍| 90/96 [03:49<00:15,  2.52s/it, training_loss=0.153]\u001b[A\n",
      "Epoch 5:  95%|█████████▍| 91/96 [03:49<00:12,  2.52s/it, training_loss=0.153]\u001b[A\n",
      "Epoch 5:  95%|█████████▍| 91/96 [03:52<00:12,  2.52s/it, training_loss=0.123]\u001b[A\n",
      "Epoch 5:  96%|█████████▌| 92/96 [03:52<00:10,  2.51s/it, training_loss=0.123]\u001b[A\n",
      "Epoch 5:  96%|█████████▌| 92/96 [03:54<00:10,  2.51s/it, training_loss=0.057]\u001b[A\n",
      "Epoch 5:  97%|█████████▋| 93/96 [03:54<00:07,  2.51s/it, training_loss=0.057]\u001b[A\n",
      "Epoch 5:  97%|█████████▋| 93/96 [03:57<00:07,  2.51s/it, training_loss=0.146]\u001b[A\n",
      "Epoch 5:  98%|█████████▊| 94/96 [03:57<00:05,  2.51s/it, training_loss=0.146]\u001b[A\n",
      "Epoch 5:  98%|█████████▊| 94/96 [03:59<00:05,  2.51s/it, training_loss=0.072]\u001b[A\n",
      "Epoch 5:  99%|█████████▉| 95/96 [03:59<00:02,  2.51s/it, training_loss=0.072]\u001b[A\n",
      "Epoch 5:  99%|█████████▉| 95/96 [04:01<00:02,  2.51s/it, training_loss=0.128]\u001b[A\n",
      "Epoch 5: 100%|██████████| 96/96 [04:01<00:00,  2.24s/it, training_loss=0.128]\u001b[A\n",
      " 80%|████████  | 4/5 [21:13<04:17, 257.47s/it]\n",
      "  0%|          | 0/17 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5\n",
      "Training loss: 0.35155836461732787\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  6%|▌         | 1/17 [00:00<00:14,  1.10it/s]\u001b[A\n",
      " 12%|█▏        | 2/17 [00:01<00:13,  1.09it/s]\u001b[A\n",
      " 18%|█▊        | 3/17 [00:02<00:12,  1.09it/s]\u001b[A\n",
      " 24%|██▎       | 4/17 [00:03<00:11,  1.09it/s]\u001b[A\n",
      " 29%|██▉       | 5/17 [00:04<00:11,  1.09it/s]\u001b[A\n",
      " 35%|███▌      | 6/17 [00:05<00:10,  1.09it/s]\u001b[A\n",
      " 41%|████      | 7/17 [00:06<00:09,  1.09it/s]\u001b[A\n",
      " 47%|████▋     | 8/17 [00:07<00:08,  1.08it/s]\u001b[A\n",
      " 53%|█████▎    | 9/17 [00:08<00:07,  1.08it/s]\u001b[A\n",
      " 59%|█████▉    | 10/17 [00:09<00:06,  1.08it/s]\u001b[A\n",
      " 65%|██████▍   | 11/17 [00:10<00:05,  1.09it/s]\u001b[A\n",
      " 71%|███████   | 12/17 [00:11<00:04,  1.09it/s]\u001b[A\n",
      " 76%|███████▋  | 13/17 [00:11<00:03,  1.09it/s]\u001b[A\n",
      " 82%|████████▏ | 14/17 [00:12<00:02,  1.09it/s]\u001b[A\n",
      " 88%|████████▊ | 15/17 [00:13<00:01,  1.09it/s]\u001b[A\n",
      " 94%|█████████▍| 16/17 [00:14<00:00,  1.09it/s]\u001b[A\n",
      "100%|██████████| 17/17 [00:15<00:00,  1.09it/s]\n",
      "100%|██████████| 5/5 [21:28<00:00, 257.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.4344688198145698\n",
      "F1 Score (Weighted): 0.8830702613271352\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in tqdm(range(1, epochs+1)):\n",
    "\n",
    "    model.train() # we set our model to train mode. (this is the mode that we train our model with train dataset)\n",
    "    \n",
    "    loss_train_total = 0\n",
    "\n",
    "    progress_bar = tqdm(dataloader_train, desc='Epoch {:1d}'.format(epoch), leave=False, disable=False)\n",
    "\n",
    "    for batch in progress_bar:\n",
    "\n",
    "        model.zero_grad()\n",
    "        \n",
    "        batch = tuple(b.to(device) for b in batch)\n",
    "        \n",
    "        # BERT's input format that I was mantioned above\n",
    "        inputs = {'input_ids':      batch[0],\n",
    "                  'attention_mask': batch[1],\n",
    "                  'labels':         batch[2],\n",
    "                 }       \n",
    "\n",
    "        outputs = model(**inputs)\n",
    "        \n",
    "        loss = outputs[0] # loss value\n",
    "        loss_train_total += loss.item() \n",
    "        loss.backward() # backpropogation\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        progress_bar.set_postfix({'training_loss': '{:.3f}'.format(loss.item()/len(batch))})\n",
    "      \n",
    "    # after this loop one epoch will be trained.\n",
    "         \n",
    "    # we save the model after each epoch, so we can use whichever we want\n",
    "    torch.save(model.state_dict(), f'finetuned_BERT_epoch_{epoch}.model')\n",
    "        \n",
    "    tqdm.write(f'\\nEpoch {epoch}')\n",
    "    \n",
    "    loss_train_avg = loss_train_total/len(dataloader_train)            \n",
    "    tqdm.write(f'Training loss: {loss_train_avg}') # writing the training loss value after each epoch\n",
    "    \n",
    "    val_loss, predictions, true_vals = evaluate(dataloader_validation) # calculating the validation loss and predicting the labels\n",
    "    val_f1 = f1_score_func(predictions, true_vals) # calculating the f1 score\n",
    "    tqdm.write(f'Validation loss: {val_loss}')  # writing the validation loss value after each epoch\n",
    "    tqdm.write(f'F1 Score (Weighted): {val_f1}') # writing the weighted f1 score value after each epoch\n",
    "\n",
    "# training begins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "Byazs5XW7k0u",
    "outputId": "2f4a0899-38b0-4360-f0fb-f11a421f5726"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# we reread our BERT model in case it has changed before\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\",\n",
    "                                                      num_labels=len(label_dict),\n",
    "                                                      output_attentions=False,\n",
    "                                                      output_hidden_states=False)\n",
    "\n",
    "model.to(device); # we send our model to device (cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "colab_type": "code",
    "id": "IMHrqIPruDwr",
    "outputId": "5ba7ea60-1eae-4af2-d332-8b67873e35d0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [00:15<00:00,  1.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Class: ekonomi\n",
      "Accuracy: 75/90\n",
      "\n",
      "Class: kultursanat\n",
      "Accuracy: 80/90\n",
      "\n",
      "Class: saglık\n",
      "Accuracy: 81/90\n",
      "\n",
      "Class: siyaset\n",
      "Accuracy: 83/90\n",
      "\n",
      "Class: spor\n",
      "Accuracy: 86/90\n",
      "\n",
      "Class: teknoloji\n",
      "Accuracy: 72/90\n",
      "\n",
      "Total accuracy:  0.8833333333333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model_name = 'finetuned_BERT_epoch_' + str(epochs) +'.model' # read the model that we saved while training, you can read any epoch\n",
    "model.load_state_dict(torch.load(model_name, map_location=torch.device('cpu')))\n",
    "\n",
    "_, predictions, true_vals = evaluate(dataloader_validation) # calculating predictions and getting true labels\n",
    "\n",
    "accuracy_per_class(predictions, true_vals) # printing the accuracy of each class and the total accuracy"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "ttc-3600_bert.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "06d1ab9e7eb44851be0ad47a96ce361b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_60db2d3a78fe4f209b2a9d43619e5b6a",
      "placeholder": "​",
      "style": "IPY_MODEL_c61c3bf3df90434abef8fc4c54a55778",
      "value": " 232k/232k [00:00&lt;00:00, 1.23MB/s]"
     }
    },
    "1bf37b95345a49a19f308c2343d8f8f4": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_be0b6b6794704b43979a4233d09dde4f",
      "max": 440473133,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_84493f2ed8e94a11bc5f543afbb297ef",
      "value": 440473133
     }
    },
    "1d25fd929cdf4032a7707c78a482b77d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_7432beddb0ab455fa488a40bf12fed60",
       "IPY_MODEL_3c0fe3a359c64eeead9b65045853c3ab"
      ],
      "layout": "IPY_MODEL_b814ccd11ed442eda2d7781658fb83b0"
     }
    },
    "2824473b348741ed8c20173586323e2a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "3350c115b9f44a4f904e8a3fffafc177": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_fce55e5590814ecc89dad4a5028b3546",
       "IPY_MODEL_06d1ab9e7eb44851be0ad47a96ce361b"
      ],
      "layout": "IPY_MODEL_9e21a408993c49c2b8fa2cc9d42ed017"
     }
    },
    "3c0fe3a359c64eeead9b65045853c3ab": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c5be857ac28943b3ab04a4c0a477c73b",
      "placeholder": "​",
      "style": "IPY_MODEL_645e3a8e8f244318aeab28394143d2eb",
      "value": " 433/433 [00:09&lt;00:00, 44.7B/s]"
     }
    },
    "3f37158ee16e489fb12c93e0608f7bb0": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_68550ac06d354677803635d6a8160e91",
      "placeholder": "​",
      "style": "IPY_MODEL_c91ff836838143d683142731ee0da983",
      "value": " 440M/440M [00:09&lt;00:00, 46.8MB/s]"
     }
    },
    "541b3d0ba5cf4091949882d30d67ea81": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_1bf37b95345a49a19f308c2343d8f8f4",
       "IPY_MODEL_3f37158ee16e489fb12c93e0608f7bb0"
      ],
      "layout": "IPY_MODEL_80fa27b4006b425fb5b7c3cd001561c9"
     }
    },
    "60db2d3a78fe4f209b2a9d43619e5b6a": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "645e3a8e8f244318aeab28394143d2eb": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "66992449b82f45918dc84e5772365d06": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "68550ac06d354677803635d6a8160e91": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7432beddb0ab455fa488a40bf12fed60": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_77850034fdb04f11a1d354ad7d21ca81",
      "max": 433,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_ebd9fa34107647bd9b73dfbc3b6b23b4",
      "value": 433
     }
    },
    "77850034fdb04f11a1d354ad7d21ca81": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "80fa27b4006b425fb5b7c3cd001561c9": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "84493f2ed8e94a11bc5f543afbb297ef": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "9e21a408993c49c2b8fa2cc9d42ed017": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b814ccd11ed442eda2d7781658fb83b0": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "be0b6b6794704b43979a4233d09dde4f": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c5be857ac28943b3ab04a4c0a477c73b": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c61c3bf3df90434abef8fc4c54a55778": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c91ff836838143d683142731ee0da983": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ebd9fa34107647bd9b73dfbc3b6b23b4": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "fce55e5590814ecc89dad4a5028b3546": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_66992449b82f45918dc84e5772365d06",
      "max": 231508,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_2824473b348741ed8c20173586323e2a",
      "value": 231508
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
